{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from ssd_utils import generate_dboxes\n",
    "from ssd_transform import SSDTransformer\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.multiprocessing.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/run/media/misha/G/mipt-dl/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_classes(the_path):\n",
    "    train_txts = glob.glob(f\"{the_path}/train/*.txt\")\n",
    "    all_labels = set()\n",
    "    for i, tx in enumerate(train_txts):\n",
    "        with open(tx, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            labels = [int(x.split(' ')[0]) for x in lines]\n",
    "            all_labels.update(labels)\n",
    "    return list(sorted(list(all_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = get_number_of_classes(DATA_DIR)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision.ops import box_convert\n",
    "class StreetDataset(Dataset):\n",
    "    def __init__(self, the_path, transformations):\n",
    "        super().__init__()\n",
    "        self.the_path = the_path\n",
    "        self.transformations = transformations\n",
    "        self._post_init()\n",
    "\n",
    "    def _post_init(self) -> None:\n",
    "        self.all_labels = glob.glob(f\"{self.the_path}/*.txt\")\n",
    "        self.all_imgs = glob.glob(f\"{self.the_path}/*.jpg\")\n",
    "        if len(self.all_labels) != len(self.all_imgs):\n",
    "            raise ValueError(\"The amount of y and amount of X are not the same\")\n",
    "\n",
    "    def _parse_labels(self, label_path, width, height):\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        nl = len(lines)\n",
    "        labels = [int(x.split(' ')[0]) for x in lines]\n",
    "        labels = torch.as_tensor(labels, dtype=torch.long)\n",
    "        bboxes = [x.split(' ')[1:] for x in lines]\n",
    "        bboxes = [[float(x.strip()) for x in row] for row in bboxes]\n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)\n",
    "\n",
    "        bboxes[..., 0] *= width\n",
    "        bboxes[..., 1] *= height\n",
    "\n",
    "        bboxes[..., 2] *= width\n",
    "        bboxes[..., 3] *= height\n",
    "        bboxes = box_convert(bboxes, 'cxcywh', 'xyxy')\n",
    "\n",
    "        return bboxes, labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.all_imgs[index]\n",
    "        img = Image.open(img_path)\n",
    "        width, height = img.size\n",
    "        img = img.resize((300, 300))\n",
    "        bboxes, labels = self._parse_labels(self.all_labels[index], 300, 300)\n",
    "        # img = cv2.imread(img_path)\n",
    "        # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        x, (height, width), boxes, labels = self.transformations(img, (300, 300), bboxes, labels)\n",
    "        return x, (300, 300), boxes, labels\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yolo_transforms():\n",
    "    train_transforms = transforms.Compose([transforms.ToTensor(), transforms.Resize((640, 640)), transforms.RandomHorizontalFlip()])\n",
    "    val_transforms = transforms.Compose([transforms.ToTensor(), transforms.Resize((640, 640))])\n",
    "    return train_transforms, val_transforms\n",
    "    \n",
    "def get_ssd_transforms(dboxes):\n",
    "    train_transforms =  SSDTransformer(dboxes, (300, 300), val=False)\n",
    "    val_transforms =  SSDTransformer(dboxes, (300, 300), val=True)\n",
    "    return train_transforms, val_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dboxes = generate_dboxes(model=\"ssd\")\n",
    "train_transforms, val_transforms = get_ssd_transforms(dboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = StreetDataset(os.path.join(DATA_DIR, 'train'), train_transforms)\n",
    "val_ds = StreetDataset(os.path.join(DATA_DIR, 'val'), val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6561e+02, 1.1595e+02, 3.3625e+00, 3.7362e+00],\n",
       "        [4.0000e-02, 1.3333e-02, 7.0000e-02, 7.0000e-02],\n",
       "        [6.6667e-02, 1.3333e-02, 7.0000e-02, 7.0000e-02],\n",
       "        ...,\n",
       "        [5.0000e-01, 5.0000e-01, 9.5577e-01, 9.5577e-01],\n",
       "        [5.0000e-01, 5.0000e-01, 1.0000e+00, 6.1518e-01],\n",
       "        [5.0000e-01, 5.0000e-01, 6.1518e-01, 1.0000e+00]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = train_ds[0][2]\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import os\n",
    "\n",
    "def ssd_collate_fn(batch):\n",
    "    items = list(zip(*batch))\n",
    "    items[0] = default_collate([i for i in items[0] if torch.is_tensor(i)])\n",
    "    items[1] = list([i for i in items[1] if i])\n",
    "    items[2] = default_collate([i for i in items[2] if torch.is_tensor(i)])\n",
    "    items[3] = default_collate([i for i in items[3] if torch.is_tensor(i)])\n",
    "    return items\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    images = []\n",
    "    bboxes = []\n",
    "\n",
    "    for b in batch:\n",
    "        images.append(b[0])\n",
    "        bboxes.append(b[1])\n",
    "\n",
    "    images = torch.stack(images, dim=0)\n",
    "    return images, bboxes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, shuffle=True, batch_size=4, num_workers=1, collate_fn=ssd_collate_fn, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, shuffle=False, batch_size=4, num_workers=1, collate_fn=ssd_collate_fn, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/misha/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/misha/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.resnet import resnet50\n",
    "\n",
    "class Base(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def init_weights(self):\n",
    "        layers = [*self.additional_blocks, *self.loc, *self.conf]\n",
    "        for layer in layers:\n",
    "            for param in layer.parameters():\n",
    "                if param.dim() > 1:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def bbox_view(self, src, loc, conf):\n",
    "        ret = []\n",
    "        for s, l, c in zip(src, loc, conf):\n",
    "            ret.append((l(s).view(s.size(0), 4, -1), c(s).view(s.size(0), self.num_classes, -1)))\n",
    "\n",
    "        locs, confs = list(zip(*ret))\n",
    "        locs, confs = torch.cat(locs, 2), torch.cat(confs, 2)\n",
    "        return locs, confs\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        backbone = resnet50(pretrained=True)\n",
    "        self.out_channels = [1024, 512, 512, 256, 256, 256]\n",
    "        self.feature_extractor = nn.Sequential(*list(backbone.children())[:7])\n",
    "\n",
    "        conv4_block1 = self.feature_extractor[-1][0]\n",
    "        conv4_block1.conv1.stride = (1, 1)\n",
    "        conv4_block1.conv2.stride = (1, 1)\n",
    "        conv4_block1.downsample[0].stride = (1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SSD(Base):\n",
    "    def __init__(self, backbone=ResNet(), num_classes=81):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_extractor = backbone\n",
    "        self.num_classes = num_classes\n",
    "        self._build_additional_features(self.feature_extractor.out_channels)\n",
    "        self.num_defaults = [4, 6, 6, 6, 4, 4]\n",
    "        self.loc = []\n",
    "        self.conf = []\n",
    "\n",
    "        for nd, oc in zip(self.num_defaults, self.feature_extractor.out_channels):\n",
    "            self.loc.append(nn.Conv2d(oc, nd * 4, kernel_size=3, padding=1))\n",
    "            self.conf.append(nn.Conv2d(oc, nd * self.num_classes, kernel_size=3, padding=1))\n",
    "\n",
    "        self.loc = nn.ModuleList(self.loc)\n",
    "        self.conf = nn.ModuleList(self.conf)\n",
    "        self.init_weights()\n",
    "\n",
    "    def _build_additional_features(self, input_size):\n",
    "        self.additional_blocks = []\n",
    "        for i, (input_size, output_size, channels) in enumerate(\n",
    "                zip(input_size[:-1], input_size[1:], [256, 256, 128, 128, 128])):\n",
    "            if i < 3:\n",
    "                layer = nn.Sequential(\n",
    "                    nn.Conv2d(input_size, channels, kernel_size=1, bias=False),\n",
    "                    nn.BatchNorm2d(channels),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(channels, output_size, kernel_size=3, padding=1, stride=2, bias=False),\n",
    "                    nn.BatchNorm2d(output_size),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                )\n",
    "            else:\n",
    "                layer = nn.Sequential(\n",
    "                    nn.Conv2d(input_size, channels, kernel_size=1, bias=False),\n",
    "                    nn.BatchNorm2d(channels),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(channels, output_size, kernel_size=3, bias=False),\n",
    "                    nn.BatchNorm2d(output_size),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                )\n",
    "\n",
    "            self.additional_blocks.append(layer)\n",
    "\n",
    "        self.additional_blocks = nn.ModuleList(self.additional_blocks)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        detection_feed = [x]\n",
    "        for l in self.additional_blocks:\n",
    "            x = l(x)\n",
    "            detection_feed.append(x)\n",
    "        locs, confs = self.bbox_view(detection_feed, self.loc, self.conf)\n",
    "        return locs, confs\n",
    "\n",
    "\n",
    "feature_maps = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    \"\"\"\n",
    "        Implements the loss as the sum of the followings:\n",
    "        1. Confidence Loss: All labels, with hard negative mining\n",
    "        2. Localization Loss: Only on positive labels\n",
    "        Suppose input dboxes has the shape 8732x4\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dboxes):\n",
    "        super(Loss, self).__init__()\n",
    "        self.scale_xy = 1.0 / dboxes.scale_xy\n",
    "        self.scale_wh = 1.0 / dboxes.scale_wh\n",
    "\n",
    "        self.sl1_loss = nn.SmoothL1Loss(reduce=False)\n",
    "        self.dboxes = nn.Parameter(dboxes(order=\"xywh\").transpose(0, 1).unsqueeze(dim=0), requires_grad=False)\n",
    "        self.con_loss = nn.CrossEntropyLoss(reduce=False)\n",
    "\n",
    "    def loc_vec(self, loc):\n",
    "        # print(self.dboxes.device)\n",
    "        # print(loc.device)\n",
    "        # print('-------')\n",
    "        gxy = self.scale_xy * (loc[:, :2, :] - self.dboxes[:, :2, :]) / self.dboxes[:, 2:, ]\n",
    "        gwh = self.scale_wh * (loc[:, 2:, :] / self.dboxes[:, 2:, :]).log()\n",
    "        return torch.cat((gxy, gwh), dim=1)\n",
    "\n",
    "    def forward(self, ploc, plabel, gloc, glabel):\n",
    "        \"\"\"\n",
    "            ploc, plabel: Nx4x8732, Nxlabel_numx8732\n",
    "                predicted location and labels\n",
    "            gloc, glabel: Nx4x8732, Nx8732\n",
    "                ground truth location and labels\n",
    "        \"\"\"\n",
    "        \n",
    "        mask = glabel > 0\n",
    "        pos_num = mask.sum(dim=1)\n",
    "\n",
    "        vec_gd = self.loc_vec(gloc)\n",
    "\n",
    "        ploc = ploc.to(device)\n",
    "        vec_gd = vec_gd.to(device)\n",
    "        # gloc = gloc.to(device)\n",
    "        # plabel = plabel.to(device)\n",
    "        # glabel = glabel.to(device)\n",
    "\n",
    "        # sum on four coordinates, and mask\n",
    "        sl1 = self.sl1_loss(ploc, vec_gd).sum(dim=1)\n",
    "        sl1 = (mask.float() * sl1).sum(dim=1)\n",
    "\n",
    "        # hard negative mining\n",
    "        con = self.con_loss(plabel, glabel)\n",
    "\n",
    "        # postive mask will never selected\n",
    "        con_neg = con.clone()\n",
    "        con_neg[mask] = 0\n",
    "        _, con_idx = con_neg.sort(dim=1, descending=True)\n",
    "        _, con_rank = con_idx.sort(dim=1)\n",
    "\n",
    "        # number of negative three times positive\n",
    "        neg_num = torch.clamp(3 * pos_num, max=mask.size(1)).unsqueeze(-1)\n",
    "        neg_mask = con_rank < neg_num\n",
    "\n",
    "        closs = (con * (mask.float() + neg_mask.float())).sum(dim=1)\n",
    "\n",
    "        # avoid no object detected\n",
    "        total_loss = sl1 + closs\n",
    "        num_mask = (pos_num > 0).float()\n",
    "        pos_num = pos_num.float().clamp(min=1e-6)\n",
    "        ret = (total_loss * num_mask / pos_num).mean(dim=0)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SSD(backbone=ResNet(), num_classes=len(classes))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/misha/.local/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 500 tensor(35084.2266, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "0 1000 tensor(44378.8789, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "0 1500 tensor(44884.4648, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "0 2000 tensor(30974.7188, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "0 2500 tensor(35461.8477, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "0 3000 tensor(36975.9648, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "0 3500 tensor(54529.6641, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "0 4000 tensor(38608.8516, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "0 4500 tensor(45602.2969, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "0 5000 tensor(41748.2617, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "1 5500 tensor(41492.6484, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "1 6000 tensor(38215.3438, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "1 6500 tensor(52145.4883, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "1 7000 tensor(42834.6562, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "1 7500 tensor(37955.7500, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "1 8000 tensor(31545.3027, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "1 8500 tensor(38205.3672, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "1 9000 tensor(42370.8047, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "1 9500 tensor(39945.9336, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "1 10000 tensor(23230.0117, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "2 10500 tensor(32381.0664, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "2 11000 tensor(35021.5391, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "2 11500 tensor(42540.9453, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "2 12000 tensor(26000.5859, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "2 12500 tensor(42986.8594, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "2 13000 tensor(24921.3438, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "2 13500 tensor(30671.2207, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "2 14000 tensor(30195.5977, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "2 14500 tensor(44335.6992, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "2 15000 tensor(29360.1973, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "3 15500 tensor(34912.6445, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "3 16000 tensor(31364.5762, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "3 16500 tensor(30195.4453, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "3 17000 tensor(41158.8008, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "3 17500 tensor(26577.1445, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "3 18000 tensor(28033.9199, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "3 18500 tensor(34232.0898, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "3 19000 tensor(34632.4453, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "3 19500 tensor(25917.0605, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "3 20000 tensor(26653.7266, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "4 20500 tensor(31257.2246, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "4 21000 tensor(43408.6758, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "4 21500 tensor(42241.5703, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "4 22000 tensor(18658.0742, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "4 22500 tensor(25083.5352, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "4 23000 tensor(28319.9062, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "4 23500 tensor(34538.8984, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "4 24000 tensor(28354.0762, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "4 24500 tensor(39350.0703, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "4 25000 tensor(18833.3320, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "5 25500 tensor(23379.4492, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "5 26000 tensor(24052.1328, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "5 26500 tensor(32475.0469, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "5 27000 tensor(19621.4043, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "5 27500 tensor(28905.5332, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "5 28000 tensor(19955.2773, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "5 28500 tensor(28666.6602, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "5 29000 tensor(18211.2148, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "5 29500 tensor(13586.6074, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "5 30000 tensor(18664.9180, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "6 30500 tensor(21131.5781, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "6 31000 tensor(26727.9473, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "6 31500 tensor(41772.4062, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "6 32000 tensor(28245.7168, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "6 32500 tensor(28889.6680, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "6 33000 tensor(37022.2617, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "6 33500 tensor(30864.9961, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "6 34000 tensor(19181.9492, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "6 34500 tensor(19397.2188, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "6 35000 tensor(24583.7910, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "7 35500 tensor(23598.9199, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "7 36000 tensor(28161.9297, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "7 36500 tensor(22535.7969, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "7 37000 tensor(22787.5293, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "7 37500 tensor(24871.3594, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "7 38000 tensor(16437.8145, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "7 38500 tensor(17078.6289, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "7 39000 tensor(25956.3984, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "7 39500 tensor(23694.3203, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "7 40000 tensor(15409.1084, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "8 40500 tensor(11688.2051, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "8 41000 tensor(14111.8535, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "8 41500 tensor(18153.7578, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "8 42000 tensor(9980.9551, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "8 42500 tensor(25854.1992, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "8 43000 tensor(29171.6660, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "8 43500 tensor(27095.2461, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "8 44000 tensor(12106.3857, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "8 44500 tensor(15122.2324, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "8 45000 tensor(14969.7520, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "9 45500 tensor(18735.6562, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "9 46000 tensor(10604.9844, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "9 46500 tensor(20063.2266, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "9 47000 tensor(21224.2207, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "9 47500 tensor(15187.1016, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "9 48000 tensor(17329.5859, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "9 48500 tensor(17198.5566, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "9 49000 tensor(10016.9766, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "9 49500 tensor(15479.8105, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "9 50000 tensor(20668.4160, device='cuda:0', grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "criterion = Loss(dboxes)\n",
    "losses = []\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-6)\n",
    "n = 0\n",
    "for epoch in range(10):\n",
    "    for img, img_size, gloc, glabel in train_loader:\n",
    "        n+=1\n",
    "        img = img.to(device)\n",
    "        # gloc = gloc.to(device)\n",
    "        glabel = glabel.to(device)\n",
    "\n",
    "        ploc, plabel = model(img)\n",
    "        ploc, plabel = ploc.float(), plabel.float()\n",
    "        ploc = ploc.to(device)\n",
    "        plabel = plabel.to(device)\n",
    "        gloc = gloc.transpose(1, 2)\n",
    "        loss = criterion(ploc, plabel, gloc, glabel)\n",
    "        if n%500==0:\n",
    "            print(epoch, n, loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dboxes.scale_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'ssd_trained.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54dd33f612a42928eaa3d2d50b3f50c5c2669e5cd5388b2a0b4a2eac894dab71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
