{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/run/media/misha/G/mipt-dl/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_classes(the_path):\n",
    "    train_txts = glob.glob(f\"{the_path}/train/*.txt\")\n",
    "    all_labels = set()\n",
    "    for i, tx in enumerate(train_txts):\n",
    "        with open(tx, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            labels = [int(x.split(' ')[0]) for x in lines]\n",
    "            all_labels.update(labels)\n",
    "    return list(sorted(list(all_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = get_number_of_classes(DATA_DIR)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorstr(*input):\n",
    "    # Colors a string https://en.wikipedia.org/wiki/ANSI_escape_code, i.e.  colorstr('blue', 'hello world')\n",
    "    *args, string = input if len(input) > 1 else ('blue', 'bold', input[0])  # color arguments, string\n",
    "    colors = {\n",
    "        'black': '\\033[30m',  # basic colors\n",
    "        'red': '\\033[31m',\n",
    "        'green': '\\033[32m',\n",
    "        'yellow': '\\033[33m',\n",
    "        'blue': '\\033[34m',\n",
    "        'magenta': '\\033[35m',\n",
    "        'cyan': '\\033[36m',\n",
    "        'white': '\\033[37m',\n",
    "        'bright_black': '\\033[90m',  # bright colors\n",
    "        'bright_red': '\\033[91m',\n",
    "        'bright_green': '\\033[92m',\n",
    "        'bright_yellow': '\\033[93m',\n",
    "        'bright_blue': '\\033[94m',\n",
    "        'bright_magenta': '\\033[95m',\n",
    "        'bright_cyan': '\\033[96m',\n",
    "        'bright_white': '\\033[97m',\n",
    "        'end': '\\033[0m',  # misc\n",
    "        'bold': '\\033[1m',\n",
    "        'underline': '\\033[4m'}\n",
    "    return ''.join(colors[x] for x in args) + f'{string}' + colors['end']\n",
    "    \n",
    "\n",
    "PREFIX = colorstr('AutoAnchor: ')\n",
    "\n",
    "def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scaleFill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return im, ratio, (dw, dh)\n",
    "\n",
    "def xywhn2xyxy(x, w=640, h=640, padw=0, padh=0):\n",
    "    # Convert nx4 boxes from [x, y, w, h] normalized to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[..., 0] = w * (x[..., 0] - x[..., 2] / 2) + padw  # top left x\n",
    "    y[..., 1] = h * (x[..., 1] - x[..., 3] / 2) + padh  # top left y\n",
    "    y[..., 2] = w * (x[..., 0] + x[..., 2] / 2) + padw  # bottom right x\n",
    "    y[..., 3] = h * (x[..., 1] + x[..., 3] / 2) + padh  # bottom right y\n",
    "    return y\n",
    "\n",
    "def xyxy2xywhn(x, w=640, h=640, clip=False, eps=0.0):\n",
    "    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] normalized where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[..., 0] = ((x[..., 0] + x[..., 2]) / 2) / w  # x center\n",
    "    y[..., 1] = ((x[..., 1] + x[..., 3]) / 2) / h  # y center\n",
    "    y[..., 2] = (x[..., 2] - x[..., 0]) / w  # width\n",
    "    y[..., 3] = (x[..., 3] - x[..., 1]) / h  # height\n",
    "    return y\n",
    "\n",
    "def check_anchor_order(m):\n",
    "    # Check anchor order against stride order for YOLOv5 Detect() module m, and correct if necessary\n",
    "    a = m.anchors.prod(-1).mean(-1).view(-1)  # mean anchor area per output layer\n",
    "    da = a[-1] - a[0]  # delta a\n",
    "    ds = m.stride[-1] - m.stride[0]  # delta s\n",
    "    if da and (da.sign() != ds.sign()):  # same order\n",
    "        print(f'{PREFIX}Reversing anchor order')\n",
    "        m.anchors[:] = m.anchors.flip(0)\n",
    "\n",
    "\n",
    "\n",
    "def check_anchors(dataset, model, thr=4.0, imgsz=640):\n",
    "    # Check anchor fit to data, recompute if necessary\n",
    "    m = model.module.model[-1] if hasattr(model, 'module') else model.model[-1]  # Detect()\n",
    "    shapes = imgsz * dataset.shapes / dataset.shapes.max(1, keepdims=True)\n",
    "    scale = np.random.uniform(0.9, 1.1, size=(shapes.shape[0], 1))  # augment scale\n",
    "    wh = torch.tensor(np.concatenate([l[:, 3:5] * s for s, l in zip(shapes * scale, dataset.labels)])).float()  # wh\n",
    "\n",
    "    def metric(k):  # compute metric\n",
    "        r = wh[:, None] / k[None]\n",
    "        x = torch.min(r, 1 / r).min(2)[0]  # ratio metric\n",
    "        best = x.max(1)[0]  # best_x\n",
    "        aat = (x > 1 / thr).float().sum(1).mean()  # anchors above threshold\n",
    "        bpr = (best > 1 / thr).float().mean()  # best possible recall\n",
    "        return bpr, aat\n",
    "\n",
    "    stride = m.stride.to(m.anchors.device).view(-1, 1, 1)  # model strides\n",
    "    anchors = m.anchors.clone() * stride  # current anchors\n",
    "    bpr, aat = metric(anchors.cpu().view(-1, 2))\n",
    "    s = f'\\n{PREFIX}{aat:.2f} anchors/target, {bpr:.3f} Best Possible Recall (BPR). '\n",
    "    if bpr > 0.98:  # threshold to recompute\n",
    "        print(f'{s}Current anchors are a good fit to dataset âœ…')\n",
    "    else:\n",
    "        print(f'{s}Anchors are a poor fit to dataset âš ï¸, attempting to improve...')\n",
    "        na = m.anchors.numel() // 2  # number of anchors\n",
    "        anchors = kmean_anchors(dataset, n=na, img_size=imgsz, thr=thr, gen=1000, verbose=False)\n",
    "        new_bpr = metric(anchors)[0]\n",
    "        if new_bpr > bpr:  # replace anchors\n",
    "            anchors = torch.tensor(anchors, device=m.anchors.device).type_as(m.anchors)\n",
    "            m.anchors[:] = anchors.clone().view_as(m.anchors)\n",
    "            check_anchor_order(m)  # must be in pixel-space (not grid-space)\n",
    "            m.anchors /= stride\n",
    "            s = f'{PREFIX}Done âœ… (optional: update model *.yaml to use these anchors in the future)'\n",
    "        else:\n",
    "            s = f'{PREFIX}Done âš ï¸ (original anchors better than new anchors, proceeding with original anchors)'\n",
    "        print(s)\n",
    "\n",
    "\n",
    "def kmean_anchors(dataset='./data/coco128.yaml', n=9, img_size=640, thr=4.0, gen=1000, verbose=True):\n",
    "    \"\"\" Creates kmeans-evolved anchors from training dataset\n",
    "\n",
    "        Arguments:\n",
    "            dataset: path to data.yaml, or a loaded dataset\n",
    "            n: number of anchors\n",
    "            img_size: image size used for training\n",
    "            thr: anchor-label wh ratio threshold hyperparameter hyp['anchor_t'] used for training, default=4.0\n",
    "            gen: generations to evolve anchors using genetic algorithm\n",
    "            verbose: print all results\n",
    "\n",
    "        Return:\n",
    "            k: kmeans evolved anchors\n",
    "\n",
    "        Usage:\n",
    "            from utils.autoanchor import *; _ = kmean_anchors()\n",
    "    \"\"\"\n",
    "    from scipy.cluster.vq import kmeans\n",
    "\n",
    "    npr = np.random\n",
    "    thr = 1 / thr\n",
    "    \n",
    "    def metric(k, wh):  # compute metrics\n",
    "        r = wh[:, None] / k[None]\n",
    "        x = torch.min(r, 1 / r).min(2)[0]  # ratio metric\n",
    "        # x = wh_iou(wh, torch.tensor(k))  # iou metric\n",
    "        return x, x.max(1)[0]  # x, best_x\n",
    "\n",
    "    def anchor_fitness(k):  # mutation fitness\n",
    "        _, best = metric(torch.tensor(k, dtype=torch.float32), wh)\n",
    "        return (best * (best > thr).float()).mean()  # fitness\n",
    "\n",
    "    def print_results(k, verbose=True):\n",
    "        k = k[np.argsort(k.prod(1))]  # sort small to large\n",
    "        x, best = metric(k, wh0)\n",
    "        bpr, aat = (best > thr).float().mean(), (x > thr).float().mean() * n  # best possible recall, anch > thr\n",
    "        s = f'{PREFIX}thr={thr:.2f}: {bpr:.4f} best possible recall, {aat:.2f} anchors past thr\\n' \\\n",
    "            f'{PREFIX}n={n}, img_size={img_size}, metric_all={x.mean():.3f}/{best.mean():.3f}-mean/best, ' \\\n",
    "            f'past_thr={x[x > thr].mean():.3f}-mean: '\n",
    "        for x in k:\n",
    "            s += '%i,%i, ' % (round(x[0]), round(x[1]))\n",
    "        if verbose:\n",
    "            print(s[:-2])\n",
    "        return k\n",
    "\n",
    "    # Get label wh\n",
    "    shapes = img_size * dataset.shapes / dataset.shapes.max(1, keepdims=True)\n",
    "    wh0 = np.concatenate([l[:, 3:5] * s for s, l in zip(shapes, dataset.labels)])  # wh\n",
    "\n",
    "    # Filter\n",
    "    i = (wh0 < 3.0).any(1).sum()\n",
    "    if i:\n",
    "        print(f'{PREFIX}WARNING âš ï¸ Extremely small objects found: {i} of {len(wh0)} labels are <3 pixels in size')\n",
    "    wh = wh0[(wh0 >= 2.0).any(1)].astype(np.float32)  # filter > 2 pixels\n",
    "    # wh = wh * (npr.rand(wh.shape[0], 1) * 0.9 + 0.1)  # multiply by random scale 0-1\n",
    "\n",
    "    # Kmeans init\n",
    "    try:\n",
    "        print(f'{PREFIX}Running kmeans for {n} anchors on {len(wh)} points...')\n",
    "        assert n <= len(wh)  # apply overdetermined constraint\n",
    "        s = wh.std(0)  # sigmas for whitening\n",
    "        k = kmeans(wh / s, n, iter=30)[0] * s  # points\n",
    "        assert n == len(k)  # kmeans may return fewer points than requested if wh is insufficient or too similar\n",
    "    except Exception:\n",
    "        print(f'{PREFIX}WARNING âš ï¸ switching strategies from kmeans to random init')\n",
    "        k = np.sort(npr.rand(n * 2)).reshape(n, 2) * img_size  # random init\n",
    "    wh, wh0 = (torch.tensor(x, dtype=torch.float32) for x in (wh, wh0))\n",
    "    k = print_results(k, verbose=False)\n",
    "\n",
    "    # Plot\n",
    "    # k, d = [None] * 20, [None] * 20\n",
    "    # for i in tqdm(range(1, 21)):\n",
    "    #     k[i-1], d[i-1] = kmeans(wh / s, i)  # points, mean distance\n",
    "    # fig, ax = plt.subplots(1, 2, figsize=(14, 7), tight_layout=True)\n",
    "    # ax = ax.ravel()\n",
    "    # ax[0].plot(np.arange(1, 21), np.array(d) ** 2, marker='.')\n",
    "    # fig, ax = plt.subplots(1, 2, figsize=(14, 7))  # plot wh\n",
    "    # ax[0].hist(wh[wh[:, 0]<100, 0],400)\n",
    "    # ax[1].hist(wh[wh[:, 1]<100, 1],400)\n",
    "    # fig.savefig('wh.png', dpi=200)\n",
    "    TQDM_BAR_FORMAT = '{l_bar}{bar:10}{r_bar}'  # tqdm bar format\n",
    "    # Evolve\n",
    "    f, sh, mp, s = anchor_fitness(k), k.shape, 0.9, 0.1  # fitness, generations, mutation prob, sigma\n",
    "    pbar = tqdm(range(gen), bar_format=TQDM_BAR_FORMAT)  # progress bar\n",
    "    for _ in pbar:\n",
    "        v = np.ones(sh)\n",
    "        while (v == 1).all():  # mutate until a change occurs (prevent duplicates)\n",
    "            v = ((npr.random(sh) < mp) * random.random() * npr.randn(*sh) * s + 1).clip(0.3, 3.0)\n",
    "        kg = (k.copy() * v).clip(min=2.0)\n",
    "        fg = anchor_fitness(kg)\n",
    "        if fg > f:\n",
    "            f, k = fg, kg.copy()\n",
    "            pbar.desc = f'{PREFIX}Evolving anchors with Genetic Algorithm: fitness = {f:.4f}'\n",
    "            if verbose:\n",
    "                print_results(k, verbose)\n",
    "\n",
    "    return print_results(k).astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreetDataset(Dataset):\n",
    "    def __init__(self, the_path, transformations):\n",
    "        super().__init__()\n",
    "        self.the_path = the_path\n",
    "        self.transformations = transformations\n",
    "        self._post_init()\n",
    "\n",
    "    def _post_init(self) -> None:\n",
    "        self.all_labels = glob.glob(f\"{self.the_path}/*.txt\")\n",
    "        self.all_imgs = glob.glob(f\"{self.the_path}/*.jpg\")\n",
    "        if len(self.all_labels) != len(self.all_imgs):\n",
    "            raise ValueError(\"The amount of y and amount of X are not the same\")\n",
    "        self.shapes = np.ones((self.__len__(), 2))\n",
    "        self.shapes *= 640\n",
    "        lls = []\n",
    "        for label_path in self.all_labels:\n",
    "            bboxes = np.array(self._parse_labels(label_path))\n",
    "            lls.append(bboxes)\n",
    "        self.labels = np.array(lls)\n",
    "\n",
    "    def _parse_labels(self, label_path):\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        nl = len(lines)\n",
    "        # labels = [int(x.split(' ')[0]) for x in lines]\n",
    "        # labels = torch.as_tensor(labels, dtype=torch.int16)\n",
    "        bboxes = [x.split(' ')[:] for x in lines]\n",
    "        bboxes = [[float(x.strip()) for x in row] for row in bboxes]\n",
    "        bboxes = np.array(bboxes, dtype=np.float32)\n",
    "        labels_out = torch.zeros((nl, 6))\n",
    "        labels_out[:, 1:] = torch.from_numpy(bboxes)\n",
    "\n",
    "        # targets = {'boxes': bboxes, 'labels': labels}\n",
    "        # print(targets)\n",
    "        # targets = [{k: torch.tensor(v) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        return labels_out\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        bboxes = self._parse_labels(self.all_labels[index])\n",
    "        img_path = self.all_imgs[index]\n",
    "        # img = Image.open(img_path)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)        \n",
    "        # print(img.shape)\n",
    "        # cv2.imwrite('image_cv.jpg', np.asarray(img))\n",
    "        # Image.fromarray(img).save('image_pil.jpg')\n",
    "        # tot = transforms.ToTensor()\n",
    "        # img = tot(img)\n",
    "        x = self.transformations(img)\n",
    "        return x, bboxes\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    train_transforms = transforms.Compose([transforms.ToTensor(), transforms.Resize((640, 640)), transforms.RandomHorizontalFlip()])\n",
    "    val_transforms = transforms.Compose([transforms.ToTensor(), transforms.Resize((640, 640))])\n",
    "    return train_transforms, val_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms, val_transforms = get_transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_341015/1474886633.py:19: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.labels = np.array(lls)\n"
     ]
    }
   ],
   "source": [
    "train_ds = StreetDataset(os.path.join(DATA_DIR, 'train'), train_transforms)\n",
    "val_ds = StreetDataset(os.path.join(DATA_DIR, 'val'), val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 2.0000, 0.4539, 0.4409, 0.0168, 0.0199],\n",
       "        [0.0000, 2.0000, 0.4329, 0.4434, 0.0168, 0.0249],\n",
       "        [0.0000, 2.0000, 0.4140, 0.4434, 0.0182, 0.0249],\n",
       "        [0.0000, 2.0000, 0.4939, 0.4297, 0.0126, 0.0174],\n",
       "        [0.0000, 2.0000, 0.5009, 0.4421, 0.0126, 0.0174],\n",
       "        [0.0000, 2.0000, 0.1562, 0.4402, 0.0434, 0.0324],\n",
       "        [0.0000, 2.0000, 0.3285, 0.4402, 0.0238, 0.0174],\n",
       "        [0.0000, 7.0000, 0.4666, 0.4228, 0.0084, 0.0224],\n",
       "        [0.0000, 7.0000, 0.5674, 0.2983, 0.0252, 0.0174],\n",
       "        [0.0000, 7.0000, 0.5422, 0.3219, 0.0813, 0.0399],\n",
       "        [0.0000, 7.0000, 0.5625, 0.4029, 0.0098, 0.0224],\n",
       "        [0.0000, 7.0000, 0.6480, 0.3854, 0.0210, 0.0473],\n",
       "        [0.0000, 7.0000, 0.7054, 0.3794, 0.0462, 0.0598],\n",
       "        [0.0000, 2.0000, 0.2305, 0.4458, 0.0406, 0.0149],\n",
       "        [0.0000, 2.0000, 0.1856, 0.4465, 0.0098, 0.0199],\n",
       "        [0.0000, 6.0000, 0.5520, 0.3865, 0.0112, 0.0125]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = train_ds[0][1]\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    images = []\n",
    "    bboxes = []\n",
    "\n",
    "    for b in batch:\n",
    "        images.append(b[0])\n",
    "        bboxes.append(b[1])\n",
    "\n",
    "    images = torch.stack(images, dim=0)\n",
    "    return images, bboxes\n",
    "\n",
    "\n",
    "def collate_fn4(batch):\n",
    "    im, label, path, shapes = zip(*batch)  # transposed\n",
    "    n = len(shapes) // 4\n",
    "    im4, label4, path4, shapes4 = [], [], path[:n], shapes[:n]\n",
    "\n",
    "    ho = torch.tensor([[0.0, 0, 0, 1, 0, 0]])\n",
    "    wo = torch.tensor([[0.0, 0, 1, 0, 0, 0]])\n",
    "    s = torch.tensor([[1, 1, 0.5, 0.5, 0.5, 0.5]])  # scale\n",
    "    for i in range(n):  # zidane torch.zeros(16,3,720,1280)  # BCHW\n",
    "        i *= 4\n",
    "        if random.random() < 0.5:\n",
    "            im1 = F.interpolate(im[i].unsqueeze(0).float(), scale_factor=2.0, mode='bilinear',\n",
    "                                align_corners=False)[0].type(im[i].type())\n",
    "            lb = label[i]\n",
    "        else:\n",
    "            im1 = torch.cat((torch.cat((im[i], im[i + 1]), 1), torch.cat((im[i + 2], im[i + 3]), 1)), 2)\n",
    "            lb = torch.cat((label[i], label[i + 1] + ho, label[i + 2] + wo, label[i + 3] + ho + wo), 0) * s\n",
    "        im4.append(im1)\n",
    "        label4.append(lb)\n",
    "\n",
    "    for i, lb in enumerate(label4):\n",
    "        lb[:, 0] = i  # add target image index for build_targets()\n",
    "\n",
    "    return torch.stack(im4, 0), torch.cat(label4, 0), path4, shapes4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, shuffle=True, batch_size=1, num_workers=4, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_ds, shuffle=False, batch_size=1, num_workers=4, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/misha/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2023-1-15 Python-3.10.8 torch-1.13.0+cu117 CUDA:0 (NVIDIA GeForce RTX 2080, 7941MiB)\n",
      "\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     40455  models.yolo.Detect                      [10, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "YOLOv5s summary: 214 layers, 7046599 parameters, 7046599 gradients\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True, autoshape=False, classes=len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'yolov5s_untrained.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.25000,  1.62500],\n",
      "         [ 2.00000,  3.75000],\n",
      "         [ 4.12500,  2.87500]],\n",
      "\n",
      "        [[ 1.87500,  3.81250],\n",
      "         [ 3.87500,  2.81250],\n",
      "         [ 3.68750,  7.43750]],\n",
      "\n",
      "        [[ 3.62500,  2.81250],\n",
      "         [ 4.87500,  6.18750],\n",
      "         [11.65625, 10.18750]]], device='cuda:0')\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.79 anchors/target, 0.430 Best Possible Recall (BPR). Anchors are a poor fit to dataset âš ï¸, attempting to improve...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mWARNING âš ï¸ Extremely small objects found: 295 of 37075 labels are <3 pixels in size\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 37075 points...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7948: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:01<00:00, 916.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 0.9933 best possible recall, 5.96 anchors past thr\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=640, metric_all=0.398/0.796-mean/best, past_thr=0.531-mean: 286,5, 202,8, 311,9, 158,17, 292,14, 311,24, 308,40, 335,68, 343,139\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone âœ… (optional: update model *.yaml to use these anchors in the future)\n",
      "tensor([[[35.74540,  0.63267],\n",
      "         [25.28660,  1.03843],\n",
      "         [38.91613,  1.07827]],\n",
      "\n",
      "        [[ 9.87360,  1.07002],\n",
      "         [18.23471,  0.90380],\n",
      "         [19.41502,  1.47753]],\n",
      "\n",
      "        [[ 9.62854,  1.24542],\n",
      "         [10.46634,  2.13949],\n",
      "         [10.71779,  4.35385]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "m = model.module.model[-1] if hasattr(model, 'module') else model.model[-1]  # Detect()\n",
    "print(m.anchors)\n",
    "check_anchors(val_ds, model)\n",
    "print(m.anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetector(nn.Module):\n",
    "\tdef __init__(self, base_model, num_classes):\n",
    "\t\tsuper(ObjectDetector, self).__init__()\n",
    "\t\t# initialize the base model and the number of classes\n",
    "\t\tself.base_model = base_model\n",
    "\t\tself.num_classes = num_classes\n",
    "\n",
    "\t\tself.regressor = nn.Sequential(\n",
    "\t\t\tnn.Linear(base_model.fc.in_features, 128),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(128, 64),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(64, 32),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(32, 4),\n",
    "\t\t\tnn.Sigmoid()\n",
    "\t\t)\n",
    "\t\tself.classifier = nn.Sequential(\n",
    "\t\t\tnn.Linear(base_model.fc.in_features, 512),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(),\n",
    "\t\t\tnn.Linear(512, 512),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(),\n",
    "\t\t\tnn.Linear(512, self.num_classes)\n",
    "\t\t)\n",
    "\t\tself.base_model.fc = nn.Identity()\n",
    "\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# pass the inputs through the base model and then obtain\n",
    "\t\t# predictions from two different branches of the network\n",
    "\t\tfeatures = self.base_model(x)\n",
    "\t\tbboxes = self.regressor(features)\n",
    "\t\tclass_logits = self.classifier(features)\n",
    "\t\treturn (bboxes, class_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import models\n",
    "# resnet = models.resnet18(pretrained=True)\n",
    "# model = ObjectDetector(resnet, 1)\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.19515], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.36874], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([3.78387], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.46075], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([3.77365], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([3.62432], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.72543], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.40383], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([3.54688], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([3.51231], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.24263], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([3.36809], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([3.18413], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([3.30956], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.09331], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([3.53503], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([3.97658], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.33579], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([3.98941], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([3.31053], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1.93938], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([3.54727], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1.89351], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1.93115], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.95098], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.42407], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([0.00916], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([0.00871], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.71850], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.33424], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.82555], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.73375], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.83193], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([3.05011], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([3.27222], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1.91283], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.42167], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1.75237], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([3.24204], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.91536], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.42113], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([0.00655], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.55288], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1.51089], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([3.12381], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([3.79227], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.25335], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.48483], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([0.00589], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.40230], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1.40593], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1.52253], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1.33815], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([0.00569], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.77197], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.45343], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.34221], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1.66317], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.22030], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([3.09412], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1.42979], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1.03778], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.45503], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.43460], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.42362], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.61828], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.17385], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([3.09305], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1.75650], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.04889], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([0.00468], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1.97774], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.46757], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.51789], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([4.10321], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.69925], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.17695], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.45173], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.57013], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1.69817], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.36535], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.03515], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([0.00431], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.13433], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1.20651], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.45539], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.02084], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1.27425], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([0.96161], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([0.00438], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1.24444], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.69401], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.62261], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([0.00430], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([3.25416], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.51014], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1.28115], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([2.39007], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([1.14689], device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from loss import ComputeLoss\n",
    "compute_loss = ComputeLoss(model)\n",
    "losses = []\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "n = 0\n",
    "for epoch in range(1):\n",
    "    for X, targets in train_loader:\n",
    "        n+=1\n",
    "        if n>=100:\n",
    "            break\n",
    "        model.zero_grad()\n",
    "        X = X.to(device)\n",
    "        outputs = model(X)\n",
    "        loss, loss_items = compute_loss(outputs, targets[0].to(device))\n",
    "        losses.append(loss.item())\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(outputs[0].shape)\n",
    "        # print(outputs[1].shape)\n",
    "        # print(outputs[2].shape)\n",
    "        # predicted_bboxes, predicted_labels, predicted_smthing = outputs[0], outputs[1], outputs[2]\n",
    "        # print(predicted_bboxes.shape)\n",
    "        # print(labels[0].shape)\n",
    "        # print(bboxes[0].shape, predicted_bboxes.shape)\n",
    "        # print(predicted_labels[1].shape)\n",
    "        # for bbox in bboxes:\n",
    "            # print(bbox.shape)\n",
    "        # for i in range(len(bboxes)):\n",
    "\n",
    "        # print(bboxes.shape)\n",
    "        # print(predicted_labels.shape)\n",
    "\n",
    "        # print(predicted_bboxes.shape, len(bboxes))\n",
    "        \n",
    "        # for i in range(len(labels)):\n",
    "        #     label = labels[i].to(device)\n",
    "        #     predicted_label = predicted_labels[i].to(device)\n",
    "        #     print(label.shape)\n",
    "        #     print(predicted_label.shape)\n",
    "        #     print(predicted_labels)\n",
    "        #     classification_loss(predicted_label, label)\n",
    "        # print(outputs)\n",
    "        # a = 5\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/misha/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2023-1-15 Python-3.10.8 torch-1.13.0+cu117 CUDA:0 (NVIDIA GeForce RTX 2080, 7941MiB)\n",
      "\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     40455  models.yolo.Detect                      [10, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "YOLOv5s summary: 214 layers, 7046599 parameters, 7046599 gradients\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_img = cv2.imread(train_ds.all_imgs[1])\n",
    "inf_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', autoshape=False, classes=len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_model.load_state_dict(torch.load('yolov5s_untrained.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[[-1.59492e+00, -1.04936e+00,  1.01636e+00,  ..., -3.70942e+00, -1.95839e+00, -2.00335e+00],\n",
       "            [-1.29904e+00, -8.57357e-01,  1.37752e+00,  ..., -3.15033e+00, -2.53233e+00, -1.92315e+00],\n",
       "            [-1.31988e+00,  3.50413e-01,  1.17287e+00,  ..., -4.09144e+00, -2.28672e+00, -1.12478e+00],\n",
       "            ...,\n",
       "            [-1.16982e+00, -1.28806e+00,  1.47513e+00,  ..., -3.95985e+00, -2.23873e+00, -2.78596e+00],\n",
       "            [-9.57968e-02, -2.23272e+00,  1.99048e+00,  ..., -3.70358e+00, -2.94877e+00, -3.19296e+00],\n",
       "            [ 6.82479e-01, -2.32830e+00,  1.60807e+00,  ..., -2.80107e+00, -2.69234e+00, -3.64964e+00]],\n",
       " \n",
       "           [[-1.49507e+00,  1.17318e-01,  7.88828e-01,  ..., -4.59834e+00, -2.99275e+00, -1.28546e+00],\n",
       "            [-7.39582e-01, -3.56187e-01,  1.00553e+00,  ..., -3.80096e+00, -3.27851e+00, -2.17852e+00],\n",
       "            [-9.78547e-01,  1.14395e+00,  5.55822e-01,  ..., -4.57600e+00, -3.03635e+00, -1.32640e+00],\n",
       "            ...,\n",
       "            [-7.09222e-01, -6.67648e-01,  1.26215e+00,  ..., -4.40014e+00, -3.13327e+00, -2.87705e+00],\n",
       "            [ 2.62641e-01, -1.46325e+00,  1.78102e+00,  ..., -4.04690e+00, -3.39536e+00, -3.47447e+00],\n",
       "            [ 7.75089e-01, -1.94925e+00,  1.53888e+00,  ..., -3.51671e+00, -3.09076e+00, -3.78256e+00]],\n",
       " \n",
       "           [[-1.70923e+00, -6.97267e-01,  8.35205e-01,  ..., -4.36014e+00, -2.93739e+00, -1.09701e+00],\n",
       "            [-1.17478e+00, -3.55969e-01,  8.06776e-01,  ..., -4.04702e+00, -2.86981e+00, -6.87490e-01],\n",
       "            [-1.32557e+00,  1.86621e+00, -4.61971e-03,  ..., -4.86652e+00, -2.55504e+00, -7.51793e-01],\n",
       "            ...,\n",
       "            [-1.10152e+00, -5.99394e-01,  7.41835e-01,  ..., -4.21332e+00, -2.85491e+00, -2.98593e+00],\n",
       "            [ 6.55252e-02, -1.18186e+00,  1.85742e+00,  ..., -4.23334e+00, -3.28492e+00, -3.17530e+00],\n",
       "            [ 4.01560e-01, -1.39015e+00,  1.34968e+00,  ..., -4.03305e+00, -2.73605e+00, -3.67512e+00]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[-1.69207e-01, -6.12078e-01,  1.41191e+00,  ..., -3.16583e+00, -2.15195e+00, -3.24870e+00],\n",
       "            [ 3.22736e-01, -4.75371e-01,  6.27508e-01,  ..., -2.62514e+00, -2.32691e+00, -2.93207e+00],\n",
       "            [ 1.04293e+00, -2.28762e-01,  7.04119e-01,  ..., -2.83853e+00, -1.70085e+00, -2.19281e+00],\n",
       "            ...,\n",
       "            [-1.27511e+00,  5.12385e-01,  7.60882e-01,  ..., -2.98627e+00, -2.17124e+00, -2.44377e+00],\n",
       "            [-9.50098e-01, -1.30466e-01,  1.72507e+00,  ..., -2.29769e+00, -2.30581e+00, -2.97729e+00],\n",
       "            [-1.34390e+00,  2.65859e-01,  1.70511e+00,  ..., -2.20103e+00, -2.75291e+00, -3.15636e+00]],\n",
       " \n",
       "           [[ 2.12915e-02, -3.45211e-01,  1.63532e+00,  ..., -3.25081e+00, -2.79513e+00, -2.58619e+00],\n",
       "            [ 4.73230e-01, -9.98318e-02,  1.03780e+00,  ..., -2.42895e+00, -2.52790e+00, -2.24339e+00],\n",
       "            [ 4.51237e-01,  3.17266e-01,  9.89254e-01,  ..., -2.89264e+00, -1.93229e+00, -2.56592e+00],\n",
       "            ...,\n",
       "            [-9.14749e-01,  1.84357e-01,  6.99383e-01,  ..., -3.30995e+00, -1.96686e+00, -3.21877e+00],\n",
       "            [-6.86590e-01, -6.23467e-01,  1.14298e+00,  ..., -2.92541e+00, -1.57070e+00, -3.50014e+00],\n",
       "            [-1.11251e+00,  2.84826e-01,  1.25427e+00,  ..., -2.81463e+00, -2.18512e+00, -3.44789e+00]],\n",
       " \n",
       "           [[-2.83889e-01, -2.12143e-01,  1.95540e+00,  ..., -3.72718e+00, -2.11706e+00, -2.16996e+00],\n",
       "            [-1.16299e-03, -1.32408e-01,  1.57137e+00,  ..., -3.34739e+00, -2.26487e+00, -2.08659e+00],\n",
       "            [-2.64368e-01,  2.93324e-01,  1.10497e+00,  ..., -3.66010e+00, -1.78776e+00, -2.25696e+00],\n",
       "            ...,\n",
       "            [-1.42984e+00,  3.78156e-01,  1.13053e+00,  ..., -3.53950e+00, -1.85657e+00, -2.88049e+00],\n",
       "            [-1.55745e+00,  1.88416e-01,  1.06695e+00,  ..., -3.92460e+00, -1.67445e+00, -2.95970e+00],\n",
       "            [-1.59714e+00,  2.80342e-01,  9.14398e-01,  ..., -3.50881e+00, -1.71319e+00, -3.10604e+00]]],\n",
       " \n",
       " \n",
       "          [[[ 2.18703e+00, -4.54634e-01,  5.06217e-01,  ..., -4.65531e+00, -2.72035e+00, -2.58081e+00],\n",
       "            [ 1.96191e+00,  2.88293e-01,  1.06795e-01,  ..., -4.24726e+00, -3.58143e+00, -2.90967e+00],\n",
       "            [ 1.72544e+00, -1.55121e-01, -2.79798e-01,  ..., -4.34555e+00, -2.73975e+00, -3.39901e+00],\n",
       "            ...,\n",
       "            [ 1.76431e+00,  9.22008e-02,  2.15365e-01,  ..., -4.48070e+00, -3.16253e+00, -3.26807e+00],\n",
       "            [ 1.76463e+00,  6.07484e-01,  1.10298e+00,  ..., -4.44758e+00, -3.63547e+00, -2.45946e+00],\n",
       "            [ 1.83883e+00,  1.04287e+00,  1.74163e+00,  ..., -4.45555e+00, -3.05323e+00, -2.33086e+00]],\n",
       " \n",
       "           [[ 2.04304e+00, -1.19040e+00,  3.85971e-02,  ..., -4.74642e+00, -1.85654e+00, -2.56029e+00],\n",
       "            [ 1.15297e+00, -4.73889e-02, -3.23679e-01,  ..., -3.69257e+00, -2.75702e+00, -2.65781e+00],\n",
       "            [ 9.66511e-01, -8.75971e-01, -4.71447e-01,  ..., -4.08405e+00, -1.96543e+00, -3.41669e+00],\n",
       "            ...,\n",
       "            [ 1.09462e+00, -6.33096e-01, -2.27406e-01,  ..., -4.02468e+00, -2.55395e+00, -3.68321e+00],\n",
       "            [ 6.86120e-01, -1.42837e-01,  4.12451e-01,  ..., -4.19383e+00, -2.69388e+00, -2.57154e+00],\n",
       "            [ 1.32049e+00,  3.56848e-01,  1.16277e+00,  ..., -3.97960e+00, -2.20046e+00, -2.45124e+00]],\n",
       " \n",
       "           [[ 1.85451e+00, -4.68932e-01, -3.47459e-02,  ..., -4.09914e+00, -1.92317e+00, -2.62110e+00],\n",
       "            [ 1.33554e+00, -2.24648e-01, -4.84712e-01,  ..., -4.24083e+00, -1.83334e+00, -2.71158e+00],\n",
       "            [ 1.43665e+00, -1.87988e+00, -1.22851e+00,  ..., -4.54842e+00, -1.28881e+00, -3.04900e+00],\n",
       "            ...,\n",
       "            [ 9.87873e-01, -3.97131e-01, -5.49436e-01,  ..., -3.77345e+00, -2.38687e+00, -3.55146e+00],\n",
       "            [ 9.98950e-01,  1.13675e-01, -5.17487e-01,  ..., -3.85537e+00, -2.22847e+00, -2.71085e+00],\n",
       "            [ 1.62014e+00,  4.67620e-02,  1.08823e-01,  ..., -3.67638e+00, -1.75348e+00, -2.78844e+00]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[ 1.74267e+00, -4.95603e-01,  8.90769e-01,  ..., -3.51099e+00, -2.58596e+00, -2.43482e+00],\n",
       "            [ 6.54020e-01, -3.90430e-01, -6.42506e-02,  ..., -3.09644e+00, -2.67694e+00, -3.19855e+00],\n",
       "            [ 7.30368e-01, -3.43420e-01, -1.04783e+00,  ..., -2.57562e+00, -2.28056e+00, -2.92236e+00],\n",
       "            ...,\n",
       "            [ 2.89507e-01, -7.72558e-01, -2.88644e-01,  ..., -2.97639e+00, -1.73699e+00, -2.85034e+00],\n",
       "            [-7.17198e-02, -4.97737e-01,  6.75588e-02,  ..., -3.03257e+00, -1.53335e+00, -2.94537e+00],\n",
       "            [ 1.16998e+00,  8.67552e-02,  8.87940e-01,  ..., -2.99606e+00, -1.22213e+00, -2.94299e+00]],\n",
       " \n",
       "           [[ 1.44325e+00, -1.60706e+00,  1.04117e+00,  ..., -2.97410e+00, -2.11204e+00, -2.48519e+00],\n",
       "            [ 2.86408e-01, -1.05461e+00, -6.86115e-02,  ..., -2.58670e+00, -1.86459e+00, -3.13818e+00],\n",
       "            [ 3.55004e-01, -1.38012e+00, -1.10887e+00,  ..., -2.35797e+00, -1.18312e+00, -2.48002e+00],\n",
       "            ...,\n",
       "            [ 3.78039e-01, -4.18434e-01,  6.50664e-02,  ..., -3.06386e+00, -1.98462e+00, -2.34964e+00],\n",
       "            [ 3.14009e-01, -3.76817e-01,  4.01092e-01,  ..., -3.29133e+00, -1.66252e+00, -2.59662e+00],\n",
       "            [ 1.42972e+00,  4.64544e-02,  1.27416e+00,  ..., -3.55978e+00, -1.32961e+00, -2.70206e+00]],\n",
       " \n",
       "           [[ 1.63782e+00, -5.83037e-01,  6.59994e-01,  ..., -3.04190e+00, -2.08031e+00, -1.89750e+00],\n",
       "            [ 8.37715e-01, -5.05500e-01, -1.44480e-01,  ..., -2.57891e+00, -1.55873e+00, -2.65203e+00],\n",
       "            [ 6.45014e-01, -7.69406e-01, -7.57857e-01,  ..., -3.13207e+00, -1.28312e+00, -2.24381e+00],\n",
       "            ...,\n",
       "            [ 6.20872e-01, -1.62868e-03,  4.84586e-02,  ..., -3.16215e+00, -1.55919e+00, -2.51500e+00],\n",
       "            [ 7.33153e-01,  5.18656e-02,  1.24705e-01,  ..., -3.49895e+00, -1.19886e+00, -2.55965e+00],\n",
       "            [ 1.63665e+00,  2.08857e-01,  1.09038e+00,  ..., -3.48988e+00, -9.87594e-01, -2.47009e+00]]],\n",
       " \n",
       " \n",
       "          [[[-4.35423e-01, -6.69043e-01, -5.97057e-01,  ..., -2.25261e+00, -2.00982e+00, -2.84657e+00],\n",
       "            [ 5.35289e-01, -9.95873e-02, -1.11626e+00,  ..., -2.72092e+00, -1.76634e+00, -2.10966e+00],\n",
       "            [-8.06154e-02, -3.33663e-01, -1.63015e+00,  ..., -1.72291e+00, -9.88761e-01, -2.30253e+00],\n",
       "            ...,\n",
       "            [-5.69590e-01,  2.87592e-02, -6.83980e-01,  ..., -2.41268e+00, -1.21891e+00, -1.99727e+00],\n",
       "            [-1.05751e+00, -2.71773e-01, -4.74355e-01,  ..., -2.46789e+00, -2.05377e+00, -2.23304e+00],\n",
       "            [-1.10998e+00, -5.86460e-01,  2.16175e-01,  ..., -3.17800e+00, -2.21339e+00, -3.08506e+00]],\n",
       " \n",
       "           [[-3.72652e-01, -1.09059e+00, -1.16329e+00,  ..., -1.18711e+00, -1.58199e+00, -2.46932e+00],\n",
       "            [ 1.31001e+00, -6.28290e-01, -1.04976e+00,  ..., -2.33283e+00, -1.86390e+00, -1.85406e+00],\n",
       "            [ 4.80229e-01, -6.07918e-01, -1.89918e+00,  ..., -1.34368e+00, -1.40385e+00, -1.97682e+00],\n",
       "            ...,\n",
       "            [ 5.19002e-01, -1.44223e-01, -8.86048e-01,  ..., -2.29945e+00, -1.52570e+00, -1.94733e+00],\n",
       "            [-3.73490e-01, -4.42743e-01, -6.98550e-01,  ..., -2.17376e+00, -1.91366e+00, -2.31501e+00],\n",
       "            [-3.30575e-01, -7.15680e-01,  8.34870e-03,  ..., -2.96534e+00, -2.09086e+00, -2.72534e+00]],\n",
       " \n",
       "           [[ 3.17765e-01, -1.49411e+00, -1.36696e+00,  ..., -1.81195e+00, -1.82954e+00, -2.89754e+00],\n",
       "            [ 7.88356e-01, -1.21369e+00, -1.65224e+00,  ..., -2.00589e+00, -1.33130e+00, -2.13037e+00],\n",
       "            [ 1.90695e-01, -1.14603e+00, -2.11600e+00,  ..., -1.10676e+00, -1.04408e+00, -2.08913e+00],\n",
       "            ...,\n",
       "            [ 1.56830e-01, -3.05860e-01, -1.23548e+00,  ..., -2.27646e+00, -1.59348e+00, -2.68140e+00],\n",
       "            [-3.41166e-01, -1.88989e-01, -1.21651e+00,  ..., -2.02452e+00, -1.55866e+00, -2.58563e+00],\n",
       "            [-2.06785e-01, -6.18226e-01, -5.55982e-01,  ..., -2.62590e+00, -1.75363e+00, -2.98354e+00]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[ 4.49375e-01, -6.78322e-01, -3.43853e-01,  ..., -2.76711e+00, -3.68202e+00, -3.67111e+00],\n",
       "            [ 7.52723e-01, -2.02440e-01, -4.64685e-01,  ..., -3.75414e+00, -2.83693e+00, -3.15151e+00],\n",
       "            [ 5.20519e-01,  4.97571e-01,  1.60843e-01,  ..., -3.92588e+00, -2.96635e+00, -2.96153e+00],\n",
       "            ...,\n",
       "            [ 1.04944e+00, -6.66599e-01, -2.46427e-01,  ..., -2.32480e+00, -2.44293e+00, -3.03184e+00],\n",
       "            [ 8.43522e-01, -2.79363e-01, -3.17591e-01,  ..., -2.25380e+00, -2.36592e+00, -2.70938e+00],\n",
       "            [ 1.26181e+00, -8.75366e-01, -2.65805e-01,  ..., -2.50761e+00, -2.42612e+00, -3.54120e+00]],\n",
       " \n",
       "           [[ 4.08036e-01, -7.69926e-01,  2.62242e-01,  ..., -3.15086e+00, -4.08681e+00, -3.08123e+00],\n",
       "            [ 1.15900e+00, -6.00573e-01, -1.47730e-01,  ..., -4.14334e+00, -3.83854e+00, -2.42672e+00],\n",
       "            [ 1.72690e+00, -6.05372e-01,  1.04474e-01,  ..., -4.17972e+00, -3.65015e+00, -2.50892e+00],\n",
       "            ...,\n",
       "            [ 5.93218e-01,  6.64248e-02,  4.39596e-01,  ..., -2.96475e+00, -2.37764e+00, -3.20720e+00],\n",
       "            [ 3.80219e-01,  3.87216e-01, -9.83978e-02,  ..., -2.52266e+00, -2.70513e+00, -2.59770e+00],\n",
       "            [ 7.86198e-01, -1.95653e-01, -4.48456e-02,  ..., -2.59379e+00, -2.61759e+00, -3.18814e+00]],\n",
       " \n",
       "           [[ 6.49452e-01, -3.02383e-01, -2.70173e-01,  ..., -3.10625e+00, -4.05245e+00, -2.86296e+00],\n",
       "            [ 1.13161e+00,  1.99587e-01, -6.02023e-01,  ..., -3.95722e+00, -3.58032e+00, -2.38687e+00],\n",
       "            [ 1.51536e+00, -5.00364e-02, -4.47605e-01,  ..., -4.02109e+00, -4.07063e+00, -1.98589e+00],\n",
       "            ...,\n",
       "            [ 8.88383e-01,  3.41305e-01,  3.99748e-01,  ..., -3.40504e+00, -3.22153e+00, -2.86035e+00],\n",
       "            [ 7.62129e-01,  5.13315e-01, -2.67824e-01,  ..., -2.26987e+00, -3.00608e+00, -2.68325e+00],\n",
       "            [ 5.53042e-01, -1.83119e-02, -1.37039e-01,  ..., -2.60990e+00, -2.88786e+00, -3.43960e+00]]]]], device='cuda:0', grad_fn=<CloneBackward0>),\n",
       " tensor([[[[[ 0.28916, -0.21708,  0.60239,  ..., -2.68843, -2.96018, -2.62482],\n",
       "            [ 0.52593, -1.02601,  1.14355,  ..., -2.23247, -2.92412, -3.55385],\n",
       "            [ 0.62151, -0.40667,  0.70393,  ..., -2.33011, -2.97153, -3.82989],\n",
       "            ...,\n",
       "            [ 0.06748, -1.22146,  0.88210,  ..., -2.36619, -2.63331, -3.34082],\n",
       "            [ 0.21712, -0.78580,  0.58783,  ..., -1.68143, -2.25798, -2.43865],\n",
       "            [-0.05565, -0.77678,  0.31829,  ..., -2.44653, -1.81242, -2.29421]],\n",
       " \n",
       "           [[ 0.09112,  0.37246,  0.63524,  ..., -2.97358, -2.40655, -2.44235],\n",
       "            [-0.24380, -0.26412,  1.17799,  ..., -3.13358, -3.15566, -2.59204],\n",
       "            [ 1.07792,  0.17977,  0.68088,  ..., -2.33687, -3.07347, -2.98646],\n",
       "            ...,\n",
       "            [ 0.33441, -0.59231,  1.13729,  ..., -1.79335, -1.82163, -3.01757],\n",
       "            [ 0.56860,  0.01605,  0.66123,  ..., -1.97309, -1.61926, -2.42041],\n",
       "            [ 0.04728, -0.10962,  0.18306,  ..., -2.84422, -1.58441, -2.23352]],\n",
       " \n",
       "           [[ 0.61180, -0.14033,  0.85579,  ..., -2.99375, -2.96386, -3.12419],\n",
       "            [ 0.66126, -0.15790,  1.33553,  ..., -2.75854, -3.22510, -3.27399],\n",
       "            [ 1.30123, -0.40935,  0.79663,  ..., -2.50314, -3.61702, -3.72308],\n",
       "            ...,\n",
       "            [-0.21499, -0.64475,  1.40323,  ..., -3.22289, -3.21106, -2.99054],\n",
       "            [ 0.72624, -0.26466,  0.87641,  ..., -2.17571, -2.62824, -3.11338],\n",
       "            [-0.05202, -0.44159,  0.31808,  ..., -2.39929, -2.31614, -2.92218]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[-0.70350, -1.07019,  0.05707,  ..., -2.35299, -3.71814, -2.86884],\n",
       "            [-0.62348, -1.17208, -0.04672,  ..., -2.02528, -3.70180, -3.16163],\n",
       "            [-0.28704, -1.16823,  0.02013,  ..., -2.06298, -3.77144, -3.09819],\n",
       "            ...,\n",
       "            [-1.21386, -0.72472, -0.00982,  ..., -2.54624, -3.32116, -2.76837],\n",
       "            [-0.81691, -0.78701,  0.17715,  ..., -2.49329, -3.42213, -2.82604],\n",
       "            [-0.86855, -0.86010,  0.20933,  ..., -2.62563, -3.37328, -2.89954]],\n",
       " \n",
       "           [[ 0.14062, -1.10973,  0.33047,  ..., -2.83250, -4.07435, -2.85709],\n",
       "            [ 0.61176, -1.20519,  0.08642,  ..., -2.10745, -3.99438, -2.67335],\n",
       "            [ 0.79395, -1.04741,  0.03256,  ..., -1.96839, -4.03552, -2.72149],\n",
       "            ...,\n",
       "            [-0.36784, -0.74664,  0.26852,  ..., -2.02702, -3.73354, -3.12716],\n",
       "            [-0.39947, -0.85959,  0.27872,  ..., -2.49600, -3.49443, -2.79876],\n",
       "            [-0.61696, -1.03721,  0.24555,  ..., -2.80800, -3.42906, -2.82339]],\n",
       " \n",
       "           [[-0.30094, -0.75267,  0.12089,  ..., -2.48641, -4.17198, -2.27249],\n",
       "            [-0.17485, -0.94292, -0.33403,  ..., -1.44113, -3.54901, -2.15847],\n",
       "            [ 0.52631, -0.48556,  0.30382,  ..., -2.18310, -3.64508, -1.86366],\n",
       "            ...,\n",
       "            [-0.28344, -0.94137,  0.37063,  ..., -1.90250, -3.67563, -2.51299],\n",
       "            [-0.29623, -0.94467,  0.20618,  ..., -2.43765, -3.62195, -2.55172],\n",
       "            [-0.50354, -1.04762,  0.18359,  ..., -2.73158, -3.38978, -2.85669]]],\n",
       " \n",
       " \n",
       "          [[[-0.36493,  0.86666, -1.05441,  ..., -2.67554, -2.68303, -2.70391],\n",
       "            [-0.45864,  1.25950, -1.09412,  ..., -2.71849, -2.95913, -2.71944],\n",
       "            [-1.11519,  1.36865, -1.01621,  ..., -2.80266, -2.76050, -3.29316],\n",
       "            ...,\n",
       "            [-1.01017,  1.70984, -1.32104,  ..., -2.99297, -2.17681, -2.85622],\n",
       "            [-0.60225,  1.04927, -1.04301,  ..., -2.49599, -1.97845, -2.61838],\n",
       "            [ 0.08127,  0.29612, -0.71113,  ..., -2.25098, -2.13043, -2.08381]],\n",
       " \n",
       "           [[-0.72716,  0.94666, -1.47346,  ..., -2.98576, -2.79296, -2.53168],\n",
       "            [-0.52680,  1.21714, -1.38067,  ..., -2.22085, -3.33969, -2.25019],\n",
       "            [-1.68389,  1.96161, -1.08677,  ..., -3.07121, -2.28078, -2.80124],\n",
       "            ...,\n",
       "            [-1.28166,  1.46555, -1.42743,  ..., -3.16065, -2.02618, -2.77061],\n",
       "            [-1.10208,  0.60640, -0.67858,  ..., -2.65755, -1.85296, -2.25719],\n",
       "            [ 0.22571,  0.20839, -0.78957,  ..., -2.48720, -1.75458, -2.25760]],\n",
       " \n",
       "           [[-1.18190,  1.30410, -1.00568,  ..., -3.67147, -3.26752, -2.33892],\n",
       "            [-0.74972,  1.83493, -1.45492,  ..., -2.12075, -2.40226, -2.72695],\n",
       "            [-2.28268,  2.87722, -1.22952,  ..., -3.99549, -2.83446, -2.02645],\n",
       "            ...,\n",
       "            [-0.92055,  1.74818, -0.37602,  ..., -3.01286, -2.77711, -1.44339],\n",
       "            [-0.88550,  0.99704, -0.51890,  ..., -2.66159, -2.20362, -1.58485],\n",
       "            [ 0.02385, -0.00572, -0.63324,  ..., -2.18566, -2.22870, -2.21031]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[ 0.09955,  0.08629, -0.05885,  ..., -2.20945, -2.50019, -3.02697],\n",
       "            [-0.14475, -0.05001, -0.05987,  ..., -2.41285, -2.85662, -2.65833],\n",
       "            [-0.21073,  0.07122, -0.25376,  ..., -2.74171, -2.73009, -2.56715],\n",
       "            ...,\n",
       "            [-0.06046, -0.17752,  0.19062,  ..., -2.48385, -2.60165, -2.13604],\n",
       "            [ 0.07823, -0.11420, -0.04747,  ..., -2.33513, -2.52692, -2.36882],\n",
       "            [ 0.29185, -0.05901, -0.10812,  ..., -2.15875, -2.58053, -2.35218]],\n",
       " \n",
       "           [[-0.08012,  0.76345, -0.52321,  ..., -2.19514, -2.72742, -2.93968],\n",
       "            [-0.75778,  0.80427, -0.06920,  ..., -2.64964, -2.57289, -2.83925],\n",
       "            [-0.73380,  0.92626, -0.15476,  ..., -2.64554, -2.74762, -2.55901],\n",
       "            ...,\n",
       "            [ 0.29176,  0.03736, -0.05818,  ..., -2.74832, -2.77067, -2.39494],\n",
       "            [ 0.16851,  0.02156, -0.18624,  ..., -2.60484, -2.50851, -2.31300],\n",
       "            [ 0.61690,  0.10855, -0.51736,  ..., -2.23392, -2.57436, -2.24469]],\n",
       " \n",
       "           [[-0.08652,  0.09563,  0.27584,  ..., -2.54559, -2.73722, -2.35155],\n",
       "            [-0.13731, -0.26786,  0.13597,  ..., -3.04289, -2.88881, -2.16464],\n",
       "            [ 0.11129,  0.36100,  0.19708,  ..., -3.02980, -3.07088, -2.03612],\n",
       "            ...,\n",
       "            [ 0.27713, -0.31715, -0.07460,  ..., -3.18578, -2.58095, -2.22108],\n",
       "            [ 0.25273,  0.03441, -0.13602,  ..., -2.43978, -2.33032, -2.40243],\n",
       "            [ 0.43309,  0.29734, -0.39035,  ..., -2.03414, -2.24362, -2.34184]]],\n",
       " \n",
       " \n",
       "          [[[-0.18746, -0.29523,  0.49889,  ..., -2.70702, -2.17136, -2.44939],\n",
       "            [-0.06027, -1.15429,  1.04075,  ..., -2.70087, -1.16307, -1.73720],\n",
       "            [ 0.21003, -0.87783,  0.96600,  ..., -3.33983, -1.56699, -1.97165],\n",
       "            ...,\n",
       "            [ 0.27624, -1.05246,  1.45264,  ..., -2.68391, -1.25582, -1.07211],\n",
       "            [-0.05330, -0.23207,  0.99099,  ..., -2.36754, -2.14196, -1.58413],\n",
       "            [-0.70899,  0.46224,  1.04545,  ..., -2.21865, -2.93543, -1.50000]],\n",
       " \n",
       "           [[ 0.24847, -0.11118,  0.34767,  ..., -2.93985, -2.37954, -2.48704],\n",
       "            [-0.71682, -0.48180,  0.02270,  ..., -3.63737, -1.77694, -1.96479],\n",
       "            [ 0.21313, -0.30887,  0.77440,  ..., -3.12449, -1.79121, -1.74602],\n",
       "            ...,\n",
       "            [ 0.31336, -0.30477,  0.74261,  ..., -2.55718, -1.83970, -1.21336],\n",
       "            [-0.29358,  0.50089,  0.72876,  ..., -2.44388, -2.66879, -1.74946],\n",
       "            [-0.62865,  1.19688,  0.95521,  ..., -2.28482, -3.48678, -1.80182]],\n",
       " \n",
       "           [[ 0.30165, -0.43020,  0.46577,  ..., -2.98169, -2.69534, -1.89407],\n",
       "            [-0.87286, -0.06022,  0.31850,  ..., -3.49616, -1.91649, -1.40260],\n",
       "            [ 0.25417, -0.92020,  0.83362,  ..., -3.28411, -1.26451, -1.32928],\n",
       "            ...,\n",
       "            [-0.28223, -0.03297,  0.78722,  ..., -2.50098, -2.27852, -1.87596],\n",
       "            [-0.23963,  0.53628,  0.80931,  ..., -2.36537, -2.97454, -1.72356],\n",
       "            [-0.11873,  1.38258,  1.02142,  ..., -2.86086, -3.19294, -1.71832]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[ 0.07172,  0.25167,  0.02492,  ..., -3.44144, -2.32080, -2.43028],\n",
       "            [-0.04753,  0.06243,  0.33353,  ..., -3.55280, -2.06222, -2.73875],\n",
       "            [-0.12986, -0.09357,  0.43734,  ..., -3.33451, -2.43839, -2.92019],\n",
       "            ...,\n",
       "            [-0.04317, -0.43797,  0.13112,  ..., -2.94579, -2.70718, -2.56897],\n",
       "            [ 0.20659, -0.21028,  0.32483,  ..., -3.11242, -2.71230, -2.02123],\n",
       "            [ 0.00572, -0.46866,  0.18882,  ..., -2.93727, -2.39384, -1.99495]],\n",
       " \n",
       "           [[-0.08570, -0.40710,  0.78967,  ..., -3.80790, -1.83235, -2.84515],\n",
       "            [-0.50811, -0.61729,  0.68797,  ..., -3.29487, -1.56604, -2.91893],\n",
       "            [-0.44588, -0.70879,  0.70827,  ..., -3.23379, -1.72471, -3.16153],\n",
       "            ...,\n",
       "            [-0.11563, -0.79449,  0.14810,  ..., -3.06538, -2.67386, -2.89025],\n",
       "            [ 0.01089, -0.43348,  0.08640,  ..., -3.26260, -2.58672, -2.59438],\n",
       "            [-0.15061, -0.67469,  0.16762,  ..., -3.00974, -2.36445, -2.42040]],\n",
       " \n",
       "           [[-0.38760,  0.17719, -0.09236,  ..., -3.72127, -1.88642, -2.63773],\n",
       "            [-1.14283,  0.15822, -0.12338,  ..., -3.53872, -1.44633, -2.53002],\n",
       "            [-1.38084, -0.14693,  0.17323,  ..., -3.22890, -1.60996, -3.21288],\n",
       "            ...,\n",
       "            [-0.65923, -0.30993, -0.11728,  ..., -3.02372, -2.37734, -2.94977],\n",
       "            [-0.10387, -0.16965,  0.03980,  ..., -3.68537, -2.15948, -2.81335],\n",
       "            [-0.10919, -0.47604,  0.27332,  ..., -3.47881, -2.26157, -2.54975]]]]], device='cuda:0', grad_fn=<CloneBackward0>),\n",
       " tensor([[[[[-8.51925e-02, -6.77350e-01,  7.98160e-02,  ..., -3.05962e+00, -3.07047e+00, -2.48602e+00],\n",
       "            [-6.33224e-01, -3.88530e-02, -2.61151e-01,  ..., -2.94111e+00, -3.06413e+00, -3.12236e+00],\n",
       "            [-2.28198e-01, -4.28073e-01, -3.00982e-01,  ..., -2.81545e+00, -2.83505e+00, -3.09052e+00],\n",
       "            ...,\n",
       "            [-3.24704e-01, -3.92797e-01,  6.62895e-01,  ..., -3.49010e+00, -3.02445e+00, -3.15167e+00],\n",
       "            [-7.52105e-02, -7.80809e-01,  3.96042e-01,  ..., -3.58842e+00, -3.00827e+00, -2.78239e+00],\n",
       "            [ 2.19629e-01, -4.07414e-01,  5.58179e-01,  ..., -2.84573e+00, -2.36175e+00, -2.14256e+00]],\n",
       " \n",
       "           [[-1.38191e-02, -8.26208e-01, -1.99399e-01,  ..., -2.89552e+00, -2.92210e+00, -2.25422e+00],\n",
       "            [-1.50699e-01, -1.69775e-01, -3.24600e-01,  ..., -2.77103e+00, -2.95506e+00, -2.86723e+00],\n",
       "            [-1.88062e-01, -3.37754e-01, -4.42764e-01,  ..., -3.12564e+00, -2.88501e+00, -2.78229e+00],\n",
       "            ...,\n",
       "            [-3.24521e-01, -7.22429e-01, -1.73127e-01,  ..., -4.15115e+00, -3.63211e+00, -2.91492e+00],\n",
       "            [-1.96216e-01, -7.23145e-01,  2.87084e-01,  ..., -4.01794e+00, -3.51405e+00, -2.58057e+00],\n",
       "            [ 2.79449e-01, -3.86694e-01, -2.19665e-01,  ..., -2.66348e+00, -2.59805e+00, -1.77899e+00]],\n",
       " \n",
       "           [[ 7.82845e-02, -8.68525e-01, -4.52994e-01,  ..., -2.76152e+00, -2.72884e+00, -2.06175e+00],\n",
       "            [-5.26256e-01, -3.75014e-01, -6.44684e-01,  ..., -3.39722e+00, -3.18418e+00, -2.70476e+00],\n",
       "            [-7.71507e-01, -3.92059e-01, -4.24026e-01,  ..., -2.91917e+00, -3.11828e+00, -3.07234e+00],\n",
       "            ...,\n",
       "            [-3.20770e-01, -1.02210e+00, -1.27499e-01,  ..., -3.89151e+00, -3.83524e+00, -2.37702e+00],\n",
       "            [ 3.03995e-01, -5.43740e-01,  5.44759e-01,  ..., -3.24527e+00, -3.50861e+00, -2.62994e+00],\n",
       "            [ 5.37525e-01, -1.31617e-01,  6.89124e-01,  ..., -2.78708e+00, -2.25409e+00, -1.74317e+00]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[ 2.29032e-01, -3.24172e-01, -4.52746e-02,  ..., -3.38806e+00, -2.98135e+00, -2.56791e+00],\n",
       "            [-9.31561e-02, -3.75152e-01, -8.98542e-02,  ..., -3.57823e+00, -2.74623e+00, -2.52922e+00],\n",
       "            [-3.28974e-01, -1.64163e-01, -1.88005e-01,  ..., -3.57568e+00, -3.07038e+00, -2.42661e+00],\n",
       "            ...,\n",
       "            [-1.18097e-01,  6.02846e-02,  3.94886e-01,  ..., -3.28811e+00, -2.92401e+00, -2.78918e+00],\n",
       "            [ 6.44336e-03, -3.29177e-01,  2.00632e-01,  ..., -2.96373e+00, -2.75837e+00, -2.88588e+00],\n",
       "            [ 9.85472e-02, -2.83863e-01,  2.05978e-01,  ..., -2.92561e+00, -2.77276e+00, -2.49870e+00]],\n",
       " \n",
       "           [[ 2.05917e-01, -3.04577e-01,  2.09251e-01,  ..., -3.24130e+00, -2.79070e+00, -2.48799e+00],\n",
       "            [-6.14732e-02, -2.08170e-01,  2.77991e-01,  ..., -3.27309e+00, -2.78174e+00, -2.39515e+00],\n",
       "            [-6.04762e-01, -5.36034e-01,  1.67143e-01,  ..., -3.44643e+00, -2.91790e+00, -2.37960e+00],\n",
       "            ...,\n",
       "            [-2.60237e-01, -6.02745e-02,  4.95056e-01,  ..., -3.11209e+00, -3.03485e+00, -2.72160e+00],\n",
       "            [-1.67926e-01, -3.96012e-01,  6.80616e-02,  ..., -2.92019e+00, -2.83372e+00, -2.75404e+00],\n",
       "            [-6.11051e-02, -3.39059e-01,  1.56287e-01,  ..., -2.71084e+00, -2.83319e+00, -2.55828e+00]],\n",
       " \n",
       "           [[ 1.25194e-01, -4.23548e-01,  5.26062e-01,  ..., -3.15795e+00, -2.81412e+00, -2.40957e+00],\n",
       "            [-1.43024e-01, -3.37104e-02,  2.26769e-01,  ..., -3.27606e+00, -2.88841e+00, -2.33017e+00],\n",
       "            [-1.62704e-01,  1.72843e-01,  1.30574e-01,  ..., -3.17324e+00, -2.87934e+00, -2.49226e+00],\n",
       "            ...,\n",
       "            [-7.59903e-02, -2.18158e-01,  6.24623e-01,  ..., -3.21473e+00, -2.75853e+00, -2.71589e+00],\n",
       "            [-1.11204e-01, -5.68371e-01,  4.11355e-01,  ..., -3.11306e+00, -2.94516e+00, -2.71747e+00],\n",
       "            [-1.83057e-01, -4.17840e-01,  3.79616e-01,  ..., -2.90805e+00, -2.78773e+00, -2.61801e+00]]],\n",
       " \n",
       " \n",
       "          [[[ 1.36941e-01, -4.45861e-01,  7.93182e-02,  ..., -2.70538e+00, -2.64893e+00, -2.27881e+00],\n",
       "            [-1.30196e-01, -8.33987e-01,  4.62655e-01,  ..., -2.78807e+00, -3.02861e+00, -2.16592e+00],\n",
       "            [-2.44483e-01, -7.82638e-01,  3.65708e-01,  ..., -2.90184e+00, -2.63213e+00, -1.89853e+00],\n",
       "            ...,\n",
       "            [-1.82715e-01, -8.70865e-01,  2.60874e-01,  ..., -2.60391e+00, -2.67756e+00, -2.50399e+00],\n",
       "            [-1.08087e-01, -3.54482e-02,  1.57992e-01,  ..., -2.30090e+00, -2.34415e+00, -2.01133e+00],\n",
       "            [ 4.26498e-01,  8.31092e-01,  2.00516e-01,  ..., -2.42350e+00, -2.73247e+00, -1.91802e+00]],\n",
       " \n",
       "           [[ 4.26696e-01, -6.52483e-01,  3.43715e-01,  ..., -2.02810e+00, -2.37607e+00, -2.06356e+00],\n",
       "            [-5.80909e-01, -6.36884e-01,  4.74254e-01,  ..., -2.51739e+00, -2.99017e+00, -2.05597e+00],\n",
       "            [-1.46997e-01, -6.61735e-01,  3.68232e-01,  ..., -2.91639e+00, -2.54022e+00, -2.11091e+00],\n",
       "            ...,\n",
       "            [ 3.60372e-01, -6.22804e-01,  4.31149e-01,  ..., -2.47248e+00, -2.35619e+00, -2.11004e+00],\n",
       "            [ 4.36967e-01, -4.44271e-01,  5.26389e-01,  ..., -2.47793e+00, -2.47443e+00, -2.01973e+00],\n",
       "            [ 9.11749e-01,  6.43658e-01,  3.11967e-01,  ..., -2.25731e+00, -2.27067e+00, -2.15104e+00]],\n",
       " \n",
       "           [[ 2.67418e-01, -4.24908e-01,  1.52440e-01,  ..., -2.49251e+00, -2.60430e+00, -2.44717e+00],\n",
       "            [-4.67828e-02, -3.59751e-01, -1.90116e-01,  ..., -2.62217e+00, -3.14852e+00, -2.62989e+00],\n",
       "            [ 4.19478e-02, -5.85442e-01, -1.93055e-01,  ..., -3.06995e+00, -2.98114e+00, -2.37964e+00],\n",
       "            ...,\n",
       "            [-1.77859e-01,  2.18104e-01,  2.22505e-01,  ..., -2.43416e+00, -2.30134e+00, -2.56321e+00],\n",
       "            [-2.21722e-01,  2.15858e-01, -2.06863e-01,  ..., -2.30088e+00, -2.94632e+00, -2.46167e+00],\n",
       "            [ 1.42128e-01,  3.79207e-01,  1.12011e-02,  ..., -2.33290e+00, -2.82188e+00, -2.34948e+00]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[ 6.42937e-01, -5.85499e-01, -7.49525e-02,  ..., -3.26588e+00, -2.77843e+00, -2.73893e+00],\n",
       "            [ 4.70217e-01, -4.57313e-01, -5.89211e-01,  ..., -3.01939e+00, -2.58652e+00, -2.62356e+00],\n",
       "            [ 8.92132e-01, -4.25100e-01, -6.28327e-01,  ..., -3.10260e+00, -3.05067e+00, -2.67680e+00],\n",
       "            ...,\n",
       "            [ 3.39281e-02, -3.36326e-01, -9.66012e-02,  ..., -2.68007e+00, -2.58157e+00, -2.70109e+00],\n",
       "            [ 2.97089e-01, -2.91282e-02, -1.09612e-01,  ..., -2.78574e+00, -2.78775e+00, -2.60725e+00],\n",
       "            [ 6.51001e-02, -3.37830e-01,  2.56029e-01,  ..., -2.85919e+00, -2.81929e+00, -2.63483e+00]],\n",
       " \n",
       "           [[ 3.54552e-01, -5.05856e-01, -2.44606e-02,  ..., -3.15232e+00, -2.78282e+00, -2.52430e+00],\n",
       "            [ 1.99279e-01, -5.36552e-01, -3.62682e-01,  ..., -2.91169e+00, -2.69221e+00, -2.55268e+00],\n",
       "            [-8.66277e-03, -3.54925e-01, -5.95030e-01,  ..., -2.70548e+00, -2.66783e+00, -2.46472e+00],\n",
       "            ...,\n",
       "            [-2.03537e-01, -3.32354e-01, -9.71930e-03,  ..., -2.54029e+00, -2.61348e+00, -2.53981e+00],\n",
       "            [-3.23217e-02, -4.27316e-01, -3.69722e-02,  ..., -2.52881e+00, -2.64676e+00, -2.63354e+00],\n",
       "            [-7.60115e-02, -6.72528e-01,  2.06242e-01,  ..., -2.79692e+00, -2.62310e+00, -2.65101e+00]],\n",
       " \n",
       "           [[ 1.29845e-01, -2.12766e-01, -1.79147e-01,  ..., -2.79098e+00, -2.75077e+00, -2.25909e+00],\n",
       "            [ 4.23609e-01, -1.89452e-01, -4.69028e-01,  ..., -2.67684e+00, -2.37236e+00, -2.60489e+00],\n",
       "            [ 6.91974e-01,  8.55908e-02, -7.03063e-01,  ..., -2.79424e+00, -2.41414e+00, -3.05084e+00],\n",
       "            ...,\n",
       "            [ 1.97136e-02, -2.08568e-01, -8.52719e-02,  ..., -2.27811e+00, -2.69152e+00, -2.52342e+00],\n",
       "            [-4.26927e-01, -4.45193e-01, -1.45377e-03,  ..., -2.91367e+00, -2.65563e+00, -2.22792e+00],\n",
       "            [-8.35553e-03, -5.45851e-01,  2.92628e-01,  ..., -2.70597e+00, -2.62369e+00, -2.45224e+00]]],\n",
       " \n",
       " \n",
       "          [[[-6.93852e-02,  1.90621e-01,  8.96845e-02,  ..., -3.06800e+00, -2.99286e+00, -2.86282e+00],\n",
       "            [-2.95093e-01,  7.08488e-01,  4.61321e-01,  ..., -2.73739e+00, -3.06916e+00, -2.66443e+00],\n",
       "            [-6.03050e-01,  6.87812e-01,  1.06627e-01,  ..., -2.68252e+00, -3.14907e+00, -3.17729e+00],\n",
       "            ...,\n",
       "            [-3.79739e-01,  4.76821e-01,  2.70175e-01,  ..., -2.85531e+00, -3.08596e+00, -2.91444e+00],\n",
       "            [-2.39217e-01,  2.46785e-01, -1.42012e-01,  ..., -2.84791e+00, -2.89223e+00, -3.05938e+00],\n",
       "            [-3.05429e-03, -2.51548e-01, -3.74935e-01,  ..., -2.51486e+00, -2.51524e+00, -3.08434e+00]],\n",
       " \n",
       "           [[-1.58343e-01,  3.83137e-02,  4.72485e-01,  ..., -3.33317e+00, -2.94309e+00, -2.36312e+00],\n",
       "            [-2.77701e-01,  7.95958e-01,  3.49883e-01,  ..., -2.70728e+00, -2.96058e+00, -2.74204e+00],\n",
       "            [-7.90644e-01,  9.31315e-01,  3.78173e-01,  ..., -2.74697e+00, -2.64775e+00, -3.03869e+00],\n",
       "            ...,\n",
       "            [-8.29240e-01,  8.57010e-01, -1.99144e-01,  ..., -2.24264e+00, -2.81995e+00, -2.71858e+00],\n",
       "            [-8.75885e-01,  3.78190e-01, -7.01289e-03,  ..., -2.12564e+00, -2.80329e+00, -3.01786e+00],\n",
       "            [ 1.28335e-01, -3.68973e-01, -1.42590e-01,  ..., -2.08335e+00, -2.29879e+00, -2.91442e+00]],\n",
       " \n",
       "           [[-7.40413e-02,  3.89315e-01,  2.00425e-01,  ..., -2.74873e+00, -3.06792e+00, -2.74719e+00],\n",
       "            [-4.79318e-01,  6.18260e-01,  4.45529e-01,  ..., -2.67945e+00, -2.57631e+00, -3.39701e+00],\n",
       "            [-2.72198e-01,  7.09172e-01,  5.51430e-01,  ..., -2.51139e+00, -2.27575e+00, -3.35317e+00],\n",
       "            ...,\n",
       "            [-5.48059e-01,  7.41155e-01, -8.00377e-02,  ..., -1.85661e+00, -2.70775e+00, -3.20955e+00],\n",
       "            [-1.25036e-01,  5.59908e-01, -4.62756e-01,  ..., -1.84537e+00, -2.72169e+00, -3.21830e+00],\n",
       "            [ 8.71289e-02, -1.01864e-01, -7.50960e-01,  ..., -2.29660e+00, -2.29439e+00, -3.03607e+00]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[-1.83409e-01,  1.42942e-01, -1.24171e-01,  ..., -3.17532e+00, -2.65653e+00, -2.86109e+00],\n",
       "            [-1.98430e-02,  1.97518e-01, -6.45305e-02,  ..., -3.30545e+00, -2.52043e+00, -2.96895e+00],\n",
       "            [-3.52438e-01,  8.18671e-01,  1.06511e-01,  ..., -3.15097e+00, -2.08141e+00, -3.25738e+00],\n",
       "            ...,\n",
       "            [-2.25797e-01,  3.88471e-01,  1.78822e-01,  ..., -3.31358e+00, -2.41444e+00, -3.01831e+00],\n",
       "            [-1.95920e-01,  6.99419e-02,  2.81898e-01,  ..., -3.21265e+00, -2.38449e+00, -2.61280e+00],\n",
       "            [-1.45507e-01,  4.37383e-01,  2.10015e-01,  ..., -3.05471e+00, -2.71692e+00, -2.79573e+00]],\n",
       " \n",
       "           [[-1.76471e-01,  5.54475e-02,  1.96893e-02,  ..., -3.23718e+00, -2.76866e+00, -2.89973e+00],\n",
       "            [ 5.39163e-02,  1.93506e-01,  5.26453e-03,  ..., -3.25474e+00, -2.66680e+00, -2.89810e+00],\n",
       "            [ 1.82228e-01,  2.97692e-01, -2.20453e-02,  ..., -3.23565e+00, -2.39107e+00, -2.92381e+00],\n",
       "            ...,\n",
       "            [-3.41972e-01,  1.84405e-01,  1.18086e-01,  ..., -3.01668e+00, -2.60782e+00, -3.25388e+00],\n",
       "            [-3.34917e-01,  9.93657e-03,  7.76444e-03,  ..., -3.18707e+00, -2.44514e+00, -3.01335e+00],\n",
       "            [-3.76642e-01,  3.56136e-01, -9.48653e-02,  ..., -3.05816e+00, -2.73970e+00, -2.93428e+00]],\n",
       " \n",
       "           [[-1.43057e-01,  1.91276e-01, -5.78834e-02,  ..., -3.29338e+00, -2.72332e+00, -2.95020e+00],\n",
       "            [-2.04999e-02,  4.41666e-01,  4.32051e-02,  ..., -2.87869e+00, -2.66062e+00, -3.22587e+00],\n",
       "            [-2.08925e-01,  5.38889e-01,  1.74013e-01,  ..., -3.02711e+00, -2.51810e+00, -3.29481e+00],\n",
       "            ...,\n",
       "            [-8.77608e-02, -7.34350e-02, -2.99193e-01,  ..., -2.53635e+00, -2.66092e+00, -3.18954e+00],\n",
       "            [-1.29633e-01, -1.48539e-02, -3.98167e-01,  ..., -2.93724e+00, -2.68577e+00, -3.00092e+00],\n",
       "            [-2.00292e-01,  3.45225e-01, -4.10467e-02,  ..., -2.85917e+00, -2.95066e+00, -2.89939e+00]]]]], device='cuda:0', grad_fn=<CloneBackward0>)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_model(train_ds[0][0].unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import DetectMultiBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dddd = torch.load(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'stride' in dddd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m os\u001b[39m.\u001b[39mmakedirs(save_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[39m# Load model\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m model \u001b[39m=\u001b[39m DetectMultiBackend(weights, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m     14\u001b[0m stride, names, pt \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mstride, model\u001b[39m.\u001b[39mnames, model\u001b[39m.\u001b[39mpt\n",
      "File \u001b[0;32m~/Coursera/mipt-dl/project/common.py:342\u001b[0m, in \u001b[0;36mDetectMultiBackend.__init__\u001b[0;34m(self, weights, device, dnn, data, fp16, fuse)\u001b[0m\n\u001b[1;32m    339\u001b[0m stride \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m  \u001b[39m# default stride\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[39mif\u001b[39;00m pt:  \u001b[39m# PyTorch\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m     model \u001b[39m=\u001b[39m attempt_load(weights \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(weights, \u001b[39mlist\u001b[39;49m) \u001b[39melse\u001b[39;49;00m w, device\u001b[39m=\u001b[39;49mdevice, inplace\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, fuse\u001b[39m=\u001b[39;49mfuse)\n\u001b[1;32m    343\u001b[0m     stride \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mint\u001b[39m(model\u001b[39m.\u001b[39mstride\u001b[39m.\u001b[39mmax()), \u001b[39m32\u001b[39m)  \u001b[39m# model stride\u001b[39;00m\n\u001b[1;32m    344\u001b[0m     names \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mnames \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m'\u001b[39m\u001b[39mmodule\u001b[39m\u001b[39m'\u001b[39m) \u001b[39melse\u001b[39;00m model\u001b[39m.\u001b[39mnames  \u001b[39m# get class names\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/experimental.py:80\u001b[0m, in \u001b[0;36mattempt_load\u001b[0;34m(weights, device, inplace, fuse)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m weights \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(weights, \u001b[39mlist\u001b[39m) \u001b[39melse\u001b[39;00m [weights]:\n\u001b[1;32m     79\u001b[0m     ckpt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(attempt_download(w), map_location\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# load\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     ckpt \u001b[39m=\u001b[39m (ckpt\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mema\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m ckpt[\u001b[39m'\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mfloat()  \u001b[39m# FP32 model\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[39m# Model compatibility updates\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(ckpt, \u001b[39m'\u001b[39m\u001b[39mstride\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model'"
     ]
    }
   ],
   "source": [
    "\n",
    "save_img = True\n",
    "is_file = False\n",
    "is_url = False\n",
    "webcam = False\n",
    "screenshot = False\n",
    "\n",
    "# Directories\n",
    "save_dir = './save_dir'\n",
    "weights = 'yolov5s_untrained.pt'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Load model\n",
    "model = DetectMultiBackend(weights, device=device)\n",
    "stride, names, pt = model.stride, model.names, model.pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
    "# Dataloader\n",
    "bs = 1  # batch_size\n",
    "if webcam:\n",
    "    view_img = check_imshow(warn=True)\n",
    "    dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n",
    "    bs = len(dataset)\n",
    "elif screenshot:\n",
    "    dataset = LoadScreenshots(source, img_size=imgsz, stride=stride, auto=pt)\n",
    "else:\n",
    "    dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n",
    "vid_path, vid_writer = [None] * bs, [None] * bs\n",
    "\n",
    "# Run inference\n",
    "model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup\n",
    "seen, windows, dt = 0, [], (Profile(), Profile(), Profile())\n",
    "for path, im, im0s, vid_cap, s in dataset:\n",
    "    with dt[0]:\n",
    "        im = torch.from_numpy(im).to(model.device)\n",
    "        im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n",
    "        im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "        if len(im.shape) == 3:\n",
    "            im = im[None]  # expand for batch dim\n",
    "\n",
    "    # Inference\n",
    "    with dt[1]:\n",
    "        visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
    "        pred = model(im, augment=augment, visualize=visualize)\n",
    "\n",
    "    # NMS\n",
    "    with dt[2]:\n",
    "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
    "\n",
    "    # Second-stage classifier (optional)\n",
    "    # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n",
    "\n",
    "    # Process predictions\n",
    "    for i, det in enumerate(pred):  # per image\n",
    "        seen += 1\n",
    "        if webcam:  # batch_size >= 1\n",
    "            p, im0, frame = path[i], im0s[i].copy(), dataset.count\n",
    "            s += f'{i}: '\n",
    "        else:\n",
    "            p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)\n",
    "\n",
    "        p = Path(p)  # to Path\n",
    "        save_path = str(save_dir / p.name)  # im.jpg\n",
    "        txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # im.txt\n",
    "        s += '%gx%g ' % im.shape[2:]  # print string\n",
    "        gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "        imc = im0.copy() if save_crop else im0  # for save_crop\n",
    "        annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
    "        if len(det):\n",
    "            # Rescale boxes from img_size to im0 size\n",
    "            det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "            # Print results\n",
    "            for c in det[:, 5].unique():\n",
    "                n = (det[:, 5] == c).sum()  # detections per class\n",
    "                s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "\n",
    "            # Write results\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                if save_txt:  # Write to file\n",
    "                    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                    line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
    "                    with open(f'{txt_path}.txt', 'a') as f:\n",
    "                        f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "\n",
    "                if save_img or save_crop or view_img:  # Add bbox to image\n",
    "                    c = int(cls)  # integer class\n",
    "                    label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\n",
    "                    annotator.box_label(xyxy, label, color=colors(c, True))\n",
    "                if save_crop:\n",
    "                    save_one_box(xyxy, imc, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)\n",
    "\n",
    "        # Stream results\n",
    "        im0 = annotator.result()\n",
    "        if view_img:\n",
    "            if platform.system() == 'Linux' and p not in windows:\n",
    "                windows.append(p)\n",
    "                cv2.namedWindow(str(p), cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)  # allow window resize (Linux)\n",
    "                cv2.resizeWindow(str(p), im0.shape[1], im0.shape[0])\n",
    "            cv2.imshow(str(p), im0)\n",
    "            cv2.waitKey(1)  # 1 millisecond\n",
    "\n",
    "        # Save results (image with detections)\n",
    "        if save_img:\n",
    "            if dataset.mode == 'image':\n",
    "                cv2.imwrite(save_path, im0)\n",
    "            else:  # 'video' or 'stream'\n",
    "                if vid_path[i] != save_path:  # new video\n",
    "                    vid_path[i] = save_path\n",
    "                    if isinstance(vid_writer[i], cv2.VideoWriter):\n",
    "                        vid_writer[i].release()  # release previous video writer\n",
    "                    if vid_cap:  # video\n",
    "                        fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "                        w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                        h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                    else:  # stream\n",
    "                        fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
    "                    save_path = str(Path(save_path).with_suffix('.mp4'))  # force *.mp4 suffix on results videos\n",
    "                    vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "                vid_writer[i].write(im0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tensor = train_ds[0][0]\n",
    "first_tensor = first_tensor.unsqueeze(0)\n",
    "first_tensor = first_tensor.to(device)\n",
    "first_output = model(first_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 80, 80, 15])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.442574153789319"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(losses[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8756494843959808"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(losses[12000:12100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.724456787109375,\n",
       " 3.9800469875335693,\n",
       " 3.890322208404541,\n",
       " 2.520813226699829,\n",
       " 3.9133477210998535,\n",
       " 3.8348779678344727,\n",
       " 3.6501495838165283,\n",
       " 3.783374071121216,\n",
       " 0.019466619938611984,\n",
       " 0.01832880824804306,\n",
       " 3.9570531845092773,\n",
       " 3.6686127185821533,\n",
       " 3.216564893722534,\n",
       " 3.660982131958008,\n",
       " 3.1975789070129395,\n",
       " 3.255096912384033,\n",
       " 2.3065571784973145,\n",
       " 1.9826027154922485,\n",
       " 2.9358198642730713,\n",
       " 3.473113775253296,\n",
       " 2.0954573154449463,\n",
       " 1.8336900472640991,\n",
       " 3.3758974075317383,\n",
       " 3.2132506370544434,\n",
       " 3.5856399536132812,\n",
       " 1.9642516374588013,\n",
       " 3.7702932357788086,\n",
       " 2.1478891372680664,\n",
       " 2.4491376876831055,\n",
       " 3.943010091781616,\n",
       " 2.756061315536499,\n",
       " 2.7761402130126953,\n",
       " 2.437709331512451,\n",
       " 2.8360986709594727,\n",
       " 0.009144200943410397,\n",
       " 2.658423900604248,\n",
       " 1.428532600402832,\n",
       " 2.3744349479675293,\n",
       " 1.0361278057098389,\n",
       " 2.5058655738830566,\n",
       " 1.6161329746246338,\n",
       " 2.889284610748291,\n",
       " 2.3266687393188477,\n",
       " 1.4848957061767578,\n",
       " 2.3071250915527344,\n",
       " 2.682664155960083,\n",
       " 2.520766496658325,\n",
       " 1.5415633916854858,\n",
       " 2.61077880859375,\n",
       " 1.6193662881851196,\n",
       " 2.512113571166992,\n",
       " 2.4502105712890625,\n",
       " 3.3667092323303223,\n",
       " 1.993929147720337,\n",
       " 3.208670139312744,\n",
       " 2.9254074096679688,\n",
       " 0.007029075641185045,\n",
       " 2.156099319458008,\n",
       " 0.007160356268286705,\n",
       " 2.1428639888763428,\n",
       " 1.4358065128326416,\n",
       " 2.171511650085449,\n",
       " 2.3510384559631348,\n",
       " 2.2281575202941895,\n",
       " 2.0748493671417236,\n",
       " 1.5627353191375732,\n",
       " 2.195302963256836,\n",
       " 2.4112014770507812,\n",
       " 0.006043185479938984,\n",
       " 1.4323416948318481,\n",
       " 3.143949031829834,\n",
       " 3.0918760299682617,\n",
       " 2.3534412384033203,\n",
       " 2.2655932903289795,\n",
       " 1.2259594202041626,\n",
       " 2.1822545528411865,\n",
       " 1.5614142417907715,\n",
       " 3.503436326980591,\n",
       " 2.503751516342163,\n",
       " 1.1849031448364258,\n",
       " 1.857961654663086,\n",
       " 1.019295334815979,\n",
       " 1.9389896392822266,\n",
       " 0.005202494561672211,\n",
       " 1.3767945766448975,\n",
       " 1.0926718711853027,\n",
       " 2.584660530090332,\n",
       " 0.005121937952935696,\n",
       " 1.1483820676803589,\n",
       " 1.6966495513916016,\n",
       " 2.8751907348632812,\n",
       " 1.82260262966156,\n",
       " 1.1510045528411865,\n",
       " 2.8011560440063477,\n",
       " 2.054088592529297,\n",
       " 1.9665502309799194,\n",
       " 1.256913423538208,\n",
       " 2.0361173152923584,\n",
       " 2.0412545204162598]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54dd33f612a42928eaa3d2d50b3f50c5c2669e5cd5388b2a0b4a2eac894dab71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
