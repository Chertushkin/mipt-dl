{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG34LB_ov1SV"
      },
      "source": [
        "<p style=\"align: center;\"><img align=center src=\"https://drive.google.com/uc?export=view&id=1I8kDikouqpH4hf7JBiSYAeNT2IO52T-T\" width=600 height=480/></p>\n",
        "<h3 style=\"text-align: center;\"><b>Школа глубокого обучения ФПМИ МФТИ</b></h3>\n",
        "\n",
        "<h3 style=\"text-align: center;\"><b>Домашнее задание. Generative adversarial networks</b></h3>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEZSpS6zv5BP"
      },
      "source": [
        "В этом домашнем задании вы обучите GAN генерировать лица людей и посмотрите на то, как можно оценивать качество генерации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "iIXHhd1ZvuSY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.transforms as tt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision.utils import save_image\n",
        "from torchvision.utils import make_grid\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "sns.set(style='darkgrid', font_scale=1.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrmSpt5e478V"
      },
      "source": [
        "## Часть 1. Подготовка данных (1 балл)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp2fR2Jd2eoh"
      },
      "source": [
        "В качестве обучающей выборки возьмем часть датасета [Flickr Faces](https://github.com/NVlabs/ffhq-dataset), который содержит изображения лиц людей в высоком разрешении (1024х1024). Оригинальный датасет очень большой, поэтому мы возьмем его часть. Скачать датасет можно [здесь](https://drive.google.com/file/d/1KWPc4Pa7u2TWekUvNu9rTSO0U2eOlZA9/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0uiO3Za40iK"
      },
      "source": [
        "Давайте загрузим наши изображения. Напишите функцию, которая строит DataLoader для изображений, при этом меняя их размер до нужного значения (размер 1024 слишком большой, поэтому мы рекомендуем взять размер 128 либо немного больше)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !ls ~/Downloads -la\n",
        "# !mv ~/Downloads/thumbnails128x128.zip ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !unzip -q thumbnails128x128.zip -d flickr_faces/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "00000\n",
            "02000\n",
            "03000\n"
          ]
        }
      ],
      "source": [
        "!ls flickr_faces/thumbnails128x128 | head -n 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_DIR = 'flickr_faces/thumbnails128x128'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dir(torchvision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "id": "CObqZZVdyyVg"
      },
      "outputs": [],
      "source": [
        "def get_dataloader(image_size, batch_size):\n",
        "    \"\"\"\n",
        "    Builds dataloader for training data.\n",
        "    Use tt.Compose and tt.Resize for transformations\n",
        "    :param image_size: height and wdith of the image\n",
        "    :param batch_size: batch_size of the dataloader\n",
        "    :returns: DataLoader object \n",
        "    \"\"\"\n",
        "    # TODO: resize images, convert them to tensors and build dataloader\n",
        "    apply_transform = torchvision.transforms.Compose(\n",
        "        [\n",
        "            torchvision.transforms.ToTensor(),\n",
        "            torchvision.transforms.transforms.Resize(image_size),\n",
        "        ]\n",
        "    )\n",
        "    ds = ImageFolder(DATA_DIR, transform=apply_transform)\n",
        "    data_loader = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "id": "iwoDGf7myHPI"
      },
      "outputs": [],
      "source": [
        "image_size = 128\n",
        "batch_size = 1\n",
        "train_loader = get_dataloader(image_size, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for batch in train_loader:\n",
        "#     print(len(batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgJiWnue5Aim"
      },
      "source": [
        "## Часть 2. Построение и обучение модели (2 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n00W_EXg72er"
      },
      "source": [
        "Сконструируйте генератор и дискриминатор. Помните, что:\n",
        "* дискриминатор принимает на вход изображение (тензор размера `3 x image_size x image_size`) и выдает вероятность того, что изображение настоящее (тензор размера 1)\n",
        "\n",
        "* генератор принимает на вход тензор шумов размера `latent_size x 1 x 1` и генерирует изображение размера `3 x image_size x image_size`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyDiscriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Conv2d(3, 8, 3), \n",
        "            nn.MaxPool2d(2), \n",
        "            nn.ReLU(), \n",
        "            nn.Conv2d(8, 16, 3), \n",
        "            nn.MaxPool2d(2), \n",
        "            nn.ReLU(), \n",
        "            nn.Conv2d(16, 32, 3),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(), \n",
        "            nn.Conv2d(32, 64, 3),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 3),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128*2*2, 1),\n",
        "            # nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {},
      "outputs": [],
      "source": [
        "discriminator = MyDiscriminator()\n",
        "discriminator = discriminator.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "metadata": {
        "id": "zLMOs5O51BdB"
      },
      "outputs": [],
      "source": [
        "class MyGenerator(nn.Module):\n",
        "    def __init__(self, latent_size, image_size):\n",
        "        super().__init__()\n",
        "        self.latent_size = latent_size\n",
        "        self.image_size = image_size\n",
        "\n",
        "        self.generator = nn.Sequential(\n",
        "            nn.ConvTranspose2d(self.latent_size, 128, 3, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, 3, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 3, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 16, 3, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 8, 3, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(8, 3, 3, stride=2, output_padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.generator(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 303,
      "metadata": {
        "id": "Qrnjt3qZ1IBj"
      },
      "outputs": [],
      "source": [
        "latent_size = 100 # choose latent size\n",
        "\n",
        "generator = MyGenerator(latent_size, image_size)\n",
        "generator = generator.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_noise = torch.randn((16, latent_size, 1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 305,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([  7.,   7.,  14.,  23.,  56.,  77., 111., 125., 165., 185., 177.,\n",
              "        200., 161., 115.,  72.,  57.,  23.,  12.,   9.,   4.]),\n",
              " array([-3.07916117, -2.77398443, -2.46880746, -2.16363072, -1.85845375,\n",
              "        -1.55327702, -1.24810016, -0.94292325, -0.63774645, -0.3325696 ,\n",
              "        -0.02739275,  0.27778411,  0.58296096,  0.88813776,  1.19331467,\n",
              "         1.49849153,  1.80366826,  2.10884523,  2.41402197,  2.71919894,\n",
              "         3.02437568]),\n",
              " <BarContainer object of 20 artists>)"
            ]
          },
          "execution_count": 305,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "points = []\n",
        "for i in range(len(random_noise)):\n",
        "    points_i = random_noise.numpy()[i].squeeze()\n",
        "    for j in range(len(points_i)):\n",
        "        points.append(points_i[j])\n",
        "plt.hist(points, bins=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDHQaIzQ0B4S"
      },
      "source": [
        "Перейдем теперь к обучению нашего GANа. Алгоритм обучения следующий:\n",
        "1. Учим дискриминатор:\n",
        "  * берем реальные изображения и присваиваем им метку 1\n",
        "  * генерируем изображения генератором и присваиваем им метку 0\n",
        "  * обучаем классификатор на два класса\n",
        "\n",
        "2. Учим генератор:\n",
        "  * генерируем изображения генератором и присваиваем им метку 0\n",
        "  * предсказываем дискриминаторором, реальное это изображение или нет\n",
        "\n",
        "\n",
        "В качестве функции потерь берем бинарную кросс-энтропию"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 306,
      "metadata": {},
      "outputs": [],
      "source": [
        "discriminator = MyDiscriminator()\n",
        "discriminator = discriminator.to(device)\n",
        "\n",
        "generator = MyGenerator(latent_size, image_size)\n",
        "generator = generator.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 307,
      "metadata": {
        "id": "H2u0HPmk3B78"
      },
      "outputs": [],
      "source": [
        "lr = 0.0001\n",
        "\n",
        "model = {\n",
        "    \"discriminator\": discriminator,\n",
        "    \"generator\": generator\n",
        "}\n",
        "\n",
        "criterion = {\n",
        "    \"discriminator\": nn.BCELoss(),\n",
        "    \"generator\": nn.BCELoss()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit(model, criterion, loader, epochs, lr):\n",
        "    optimizer = {\n",
        "        'discriminator': torch.optim.SGD(model['discriminator'].parameters(), lr=lr, momentum=0.9, nesterov=True)\n",
        "    }\n",
        "    model['discriminator'].train()\n",
        "\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        for X, _ in loader:\n",
        "            optimizer['discriminator'].zero_grad()\n",
        "            batch_size = len(X)\n",
        "            real_y = torch.ones(batch_size).to(device)\n",
        "            fake_y = torch.zeros(batch_size).to(device)\n",
        "\n",
        "            real_images = X.to(device)\n",
        "            random_noise = torch.randn((batch_size, latent_size, 1, 1)).to(device)\n",
        "            fake_images = model['generator'](random_noise)\n",
        "            \n",
        "            the_batch = torch.concat([real_images, fake_images])\n",
        "            the_y = torch.concat([real_y, fake_y])\n",
        "\n",
        "            probs = model['discriminator'](the_batch)\n",
        "            loss = criterion['discriminator'](the_y, probs)\n",
        "\n",
        "            print(probs)\n",
        "            print(the_y)\n",
        "            print('------------------')\n",
        "            print(loss)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer['discriminator'].step()\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 309,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Using a target size (torch.Size([2, 1])) that is different to the input size (torch.Size([2])) is deprecated. Please ensure they have the same size.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[309], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fit(model, criterion, train_loader, \u001b[39m1\u001b[39;49m, lr)\n",
            "Cell \u001b[0;32mIn[308], line 23\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(model, criterion, loader, epochs, lr)\u001b[0m\n\u001b[1;32m     20\u001b[0m the_y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconcat([real_y, fake_y])\n\u001b[1;32m     22\u001b[0m probs \u001b[39m=\u001b[39m model[\u001b[39m'\u001b[39m\u001b[39mdiscriminator\u001b[39m\u001b[39m'\u001b[39m](the_batch)\n\u001b[0;32m---> 23\u001b[0m loss \u001b[39m=\u001b[39m criterion[\u001b[39m'\u001b[39;49m\u001b[39mdiscriminator\u001b[39;49m\u001b[39m'\u001b[39;49m](the_y, probs)\n\u001b[1;32m     25\u001b[0m \u001b[39mprint\u001b[39m(probs)\n\u001b[1;32m     26\u001b[0m \u001b[39mprint\u001b[39m(the_y)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:619\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 619\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:3086\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3084\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3085\u001b[0m \u001b[39mif\u001b[39;00m target\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize():\n\u001b[0;32m-> 3086\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3087\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing a target size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) that is different to the input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) is deprecated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3088\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure they have the same size.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m   3089\u001b[0m     )\n\u001b[1;32m   3091\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3092\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n",
            "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([2, 1])) that is different to the input size (torch.Size([2])) is deprecated. Please ensure they have the same size."
          ]
        }
      ],
      "source": [
        "fit(model, criterion, train_loader, 1, lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkecCSn69DLe"
      },
      "source": [
        "Постройте графики лосса для генератора и дискриминатора. Что вы можете сказать про эти графики?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuL3ZZvX5G29"
      },
      "source": [
        "## Часть 3. Генерация изображений (1 балл)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q9_WFIl-Bf6"
      },
      "source": [
        "Теперь давайте оценим качество получившихся изображений. Напишите функцию, которая выводит изображения, сгенерированные нашим генератором"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1tuaMVu1Jqx"
      },
      "outputs": [],
      "source": [
        "n_images = 4\n",
        "\n",
        "fixed_latent = torch.randn(n_images, latent_size, 1, 1, device=device)\n",
        "fake_images = model[\"generator\"](fixed_latent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-ZmT6qm4ai5"
      },
      "outputs": [],
      "source": [
        "def show_images(generated):\n",
        "  # TODO: show generated images\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHqPK3xs-Z-7"
      },
      "source": [
        "Как вам качество получившихся изображений?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0z41dA05KAa"
      },
      "source": [
        "## Часть 4. Leave-one-out-1-NN classifier accuracy (6 баллов)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C9V8DHX_ipy"
      },
      "source": [
        "### 4.1. Подсчет accuracy (4 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wT2uUb4_rku"
      },
      "source": [
        "Не всегда бывает удобно оценивать качество сгенерированных картинок глазами. В качестве альтернативы вам предлагается реализовать следующий подход:\n",
        "  * Сгенерировать столько же фейковых изображений, сколько есть настоящих в обучающей выборке. Присвоить фейковым метку класса 0, настоящим – 1.\n",
        "  * Построить leave-one-out оценку: обучить 1NN Classifier (`sklearn.neighbors.KNeighborsClassifier(n_neighbors=1)`) предсказывать класс на всех объектах, кроме одного, проверить качество (accuracy) на оставшемся объекте. В этом вам поможет `sklearn.model_selection.LeaveOneOut`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsrgX9X4BfE0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRU47nCzCVnP"
      },
      "source": [
        "Что вы можете сказать о получившемся результате? Какой accuracy мы хотели бы получить и почему?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqzHnPOACgoZ"
      },
      "source": [
        "### 4.2. Визуализация распределений (2 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EweiItWFDYO0"
      },
      "source": [
        "Давайте посмотрим на то, насколько похожи распределения настоящих и фейковых изображений. Для этого воспользуйтесь методом, снижающим размерность (к примеру, TSNE) и изобразите на графике разным цветом точки, соответствующие реальным и сгенерированным изображенияи"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZBJWkWdCepj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVZe9tt8DuYh"
      },
      "source": [
        "Прокомментируйте получившийся результат:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z__a1XTPEKaa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "54dd33f612a42928eaa3d2d50b3f50c5c2669e5cd5388b2a0b4a2eac894dab71"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
